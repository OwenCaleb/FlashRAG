{"id": "page-0", "contents": "RoboTwin 2.0 - RoboTwin 2.0 Offical Document\nSkip to content\n# RoboTwin 2.0 ¶\n### Webpage | PDF | arXiv | Repo | Talk (in Chinese) | LeaderBoard\nHere is the official documentation for RoboTwin 2.0, which includes installation and usage instructions for various RoboTwin functionalities, detailed information on the 50 bimanual tasks in RoboTwin 2.0, comprehensive descriptions of the RoboTwin-OD dataset, and guidelines for joining the community.\n## 1. Everything about RoboTwin 2.0 ¶\nRoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation\nTianxing Chen\n*\n, Zanxin Chen\n*\n, Baijun Chen\n*\n, Zijian Cai\n*\n,\nYibin Liu\n*\n,\nQiwei Liang\n, Zixuan Li, Xianliang Lin,\nYiheng Ge\n, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie,\nQiangyu Chen\n, Kailun Su, Tianling Xu, Guodong Liu,\nMengkang Hu\n,\nHuan-ang Gao\n, Kaixuan Wang,\nZhixuan Liang\n,\nYusen Qin\n, Xiaokang Yang,\nPing Luo\n†\n,\nYao Mu\n†\n## 2. Previous Works ¶\n[Under Review]\nRoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation\n[CVPR 2025 Highlight]\nRoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins\n[CVPR 2025 Challenge@MEIS Workshop]\nBenchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop\n[ECCV 2024 MAAS Workshop Best Paper]\nRoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)\n[第十九届挑战杯官方赛题]\n赛题链接\n## 3. Citations ¶\nIf you find our work useful, please consider citing:\nRoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation\n@article{chen2025robotwin,\ntitle={RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation},\nauthor={Chen, Tianxing and Chen, Zanxin and Chen, Baijun and Cai, Zijian and Liu, Yibin and Liang, Qiwei and Li, Zixuan and Lin, Xianliang and Ge, Yiheng and Gu, Zhenyu and others},\njournal={arXiv preprint arXiv:2506.18088},\nyear={2025}\n}\nRoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins, accepted to\nCVPR 2025 (Highlight)\n@InProceedings{Mu_2025_CVPR,\nauthor = {Mu, Yao and Chen, Tianxing and Chen, Zanxin and Peng, Shijia and Lan, Zhiqian and Gao, Zeyu and Liang, Zhixuan and Yu, Qiaojun and Zou, Yude and Xu, Mingkun and Lin, Lunkai and Xie, Zhiqiang and Ding, Mingyu and Luo, Ping},\ntitle = {RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins},\nbooktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\nmonth = {June},\nyear = {2025},\npages = {27649-27660}\n}\nBenchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop\n@article{chen2025benchmarking,\ntitle={Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop},\nauthor={Chen, Tianxing and Wang, Kaixuan and Yang, Zhaohui and Zhang, Yuhao and Chen, Zanxin and Chen, Baijun and Dong, Wanxi and Liu, Ziyuan and Chen, Dong and Yang, Tianshuo and others},\njournal={arXiv preprint arXiv:2506.23351},\nyear={2025}\n}\nRoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (\nearly version\n), accepted to\nECCV Workshop 2024 (Best Paper)\n@article{mu2024robotwin,\ntitle={RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)},\nauthor={Mu, Yao and Chen, Tianxing and Peng, Shijia and Chen, Zanxin and Gao, Zeyu and Zou, Yude and Lin, Lunkai and Xie, Zhiqiang and Luo, Ping},\njournal={arXiv preprint arXiv:2409.02920},\nyear={2024}\n}\n## 4. Contact ¶\nContact\nTianxing Chen\nif you have any questions or suggestions."}
{"id": "page-2", "contents": "RoboTwin 2.0 Usage Guide - RoboTwin 2.0 Offical Document\nSkip to content\n# RoboTwin 2.0 Usage Guide ¶\nThis documentation provides a comprehensive guide to using RoboTwin 2.0, covering environment setup, data collection and configuration, policy deployment, usage of demo policies, automatic code generation for new tasks, API tutorial, language instruction generation, and digital asset annotation."}
{"id": "page-3", "contents": "Common Issue - RoboTwin 2.0 Offical Document\nSkip to content\n# Common Issue ¶\n## 1. Modify clear_cache_freq to Reduce GPU Memory Pressure ¶\nIf you find that your GPU memory is insufficient—especially if the program consistently runs out of memory after several episodes (during data collection or evaluation)—try adjusting the\nclear_cache_freq\nparameter in the corresponding task configuration. Note that setting a smaller\nclear_cache_freq\nvalue can reduce GPU memory usage but may also slow down runtime performance (For more details, see\nConfigurations\n).\n## 2. [svulkan2] [error] OIDN Error: invalid handle ¶\nTry [\nSAPIEN issue: https://github.com/haosulab/SAPIEN/issues/243\n], or modify the\nclear_cache_freq\ninto\n1\n.\n## 3. Stuck While Collecting Data and Evaluating ¶\nPlease check your GPU model. According to user feedback and known issues reported on SAPIEN [\nSAPIEN issue: https://github.com/haosulab/SAPIEN/issues/219\n], Hopper/Ampere series GPUs (e.g., A100, H100) may occasionally experience unexpected hangs during data collection.\nYou can run\npython data/process_stuck.py ${task_name} ${task_config} ${seed_id}\nto skip the specific seed and rerun the data collection script. You can identify the\n${seed_id}\nfrom the terminal output. For example, if you get stuck at\nsaving: episode = 478 index = 105\n, then\n478\nis the\n${seed_id}\n. This will replace the affected seed (in\ndata/${task_name}/${task_config}/seed.txt\n) and trajectory data (in\ndata/${task_name}/${task_config}/_traj_data/\n) with the last seed and episode data.\nIMPORTANT:\nthis may result in insufficient episode data, so please modify the\nepisode_num\nparameter in the task config to collect more seeds and ensure the final dataset has enough episodes.\n## 4. 50 Series GPU ¶\nSee\nissue\n.\n## 5. Join the RoboTwin Community ¶\nConsider joining the\nWeChat Community\nto stay connected and receive updates."}
{"id": "page-4", "contents": "50 RoboTwin 2.0 Tasks - RoboTwin 2.0 Offical Document\nSkip to content\n# 50 RoboTwin 2.0 Tasks ¶\nThis document introduces the 50 bimanual manipulation tasks in RoboTwin 2.0, including task videos, descriptions, average step lengths, involved objects, and success rates across different robot embodiments.\nBuilding on our automated task generation framework, embodiment-adaptive behavior synthesis, and the large-scale object asset library RoboTwin-OD, we construct a suite of over 50 dual-arm collaborative manipulation tasks. In addition, we support data collection and evaluation across 5 distinct robot platforms, enabling comprehensive benchmarking of manipulation policies. The complete task set is available at\nhttp://robotwin-platform.github.io/doc/tasks/\n."}
{"id": "page-5", "contents": "RoboTwin-OD (RoboTwin Object Dataset) - RoboTwin 2.0 Offical Document\nSkip to content\n# RoboTwin-OD (RoboTwin Object Dataset) ¶\nThis is a document containing images and ID information for all objects in RoboTwin-OD, excluding those from the Objaverse dataset.\nTo enhance both manipulation capability and visual understanding, we construct a large-scale object dataset with rich semantic annotations, called\nRoboTwin-OD\n, covering 147 categories and 731 diverse objects. Specifically, this includes 534 instances across 111 categories with custom-generated and optimized meshes, 153 objects from 27 categories in Objaverse, and 44 articulated object instances from 9 categories in SAPIEN PartNet-Mobility. Objects from all sources, including Objaverse, are used for cluttered scene construction, with Objaverse specifically serving to further increase the visual and semantic diversity of distractor objects. Additionally, we develop a comprehensive surface and background texture library using generative AI and human-in-the-loop verification to ensure both diversity and realism. The dataset is available at\nhttps://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects\n."}
{"id": "page-6", "contents": "WeChat Group - RoboTwin 2.0 Offical Document\nSkip to content\n# WeChat Group ¶\nContact\nTianxing Chen\nif you have any questions or suggestions."}
{"id": "page-7", "contents": "Contributions - RoboTwin 2.0 Offical Document\nSkip to content\n# Contributions\n## Project Leaders ¶\nTianxing Chen, Yao Mu, Zhixuan Liang\n## 1. Roadmap & Methodology ¶\nYao Mu, Tianxing Chen, Ping Luo, Yusen Qin, Xiaokang Yang, Kaixuan Wang\n## 2. Data Generator & Benchmark ¶\nTianxing Chen, Zanxin Chen, Baijun Chen, Qiwei Liang, Zixuan Li, Xianliang Lin\n## 3. CodeGen Agent ¶\nYibin Liu, Zanxin Chen, Yiheng Ge, Tianxing Chen, Mengkang Hu\n## 4. RoboTwin-OD ¶\nBaijun Chen, Qiangyu Chen, Kailun Su, Xuanbing Xie, Zanxin Chen\n## 5. Policies Training & Evaluation ¶\nTianxing Chen, Zijian Cai, Tian Nian, Huan-ang Gao, Tianling Xu\n## 6. Real-World Deployment ¶\nTianxing Chen, Tian Nian, Weiliang Deng\n## 7. Domain Randomization ¶\nBaijun Chen, Yubin Guo, Qiwei Liang, Zhenyu Gu, Guodong Liu, Zanxin Chen, Tianxing Chen"}
{"id": "page-8", "contents": "Install & Download - RoboTwin 2.0 Offical Document\nSkip to content\n# Install & Download ¶\n## 1. Dependencies ¶\nSystem Support:\nWe currently best support Linux based systems. There is limited support for windows and no support for MacOS at the moment. We are working on trying to support more features on other systems but this may take some time. Most constraints stem from what the\nSAPIEN\npackage is capable of supporting.\nSystem / GPU\nCPU Sim\nGPU Sim\nRendering\nLinux / NVIDIA GPU\n✅\n✅\n✅\nWindows / NVIDIA GPU\n✅\n❌\n✅\nWindows / AMD GPU\n✅\n❌\n✅\nWSL / Anything\n✅\n❌\n❌\nMacOS / Anything\n✅\n❌\n✅\nOccasionally, data collection may get stuck when using A/H series GPUs. This issue may be related to\nRoboTwin issue #83\nand\nSAPIEN issue #219\n.\nPython versions:\nPython 3.10\nCUDA version:\n12.1 (Recommended)\nHardware:\nRendering: NVIDIA or AMD GPU\nRay tracing: NVIDIA RTX GPU or AMD equivalent\nRay-tracing Denoising: NVIDIA GPU\nGPU Simulation: NVIDIA GPU\nSoftware:\nRay tracing: NVIDIA Driver >= 470\nDenoising (OIDN): NVIDIA Driver >= 520\n### 1.1 Additional Requirements for Docker ¶\nWhen running in a Docker container, ensure that the following environment variable is set when starting the container:\n-e\nNVIDIA_DRIVER_CAPABILITIES\n=\ncompute,utility,graphics\nImportant : The graphics capability is essential. Omitting it may result in segmentation faults due to missing Vulkan support.\nFor more information, see\nHERE\n.\n## 2. Install Vulkan (if not installed) ¶\nsudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools\nCheck by running\nvulkaninfo\n## 3. Basic Env ¶\nFirst, prepare a conda environment.\nconda\ncreate\n-n\nRoboTwin\npython\n=\n3\n.10\n-y\nconda\nactivate\nRoboTwin\nRoboTwin 2.0 Code Repo:\nhttps://github.com/RoboTwin-Platform/RoboTwin\ngit\nclone\nhttps://github.com/RoboTwin-Platform/RoboTwin.git\nThen, run\nscript/_install.sh\nto install basic envs and CuRobo:\nbash script/_install.sh\nIf you meet curobo config path issue, try to run\npython script/update_embodiment_config_path.py\nIf you encounter any problems, please refer to the\nmanual installation\nsection. If you are not using 3D data, a failed installation of pytorch3d will not affect the functionality of the project.\nIf you haven't installed ffmpeg, please turn to\nhttps://ffmpeg.org/\n. Check it by running\nffmpeg -version\n.\n## 4. Download Assets (RoboTwin-OD, Texture Library and Embodiments) ¶\nTo download the assets, run the following command. If you encounter any rate-limit issues, please log in to your Hugging Face account by running\nhuggingface-cli login\n:\nbash script/_download_assets.sh\nThe structure of the\nassets\nfolder should be like this:\nassets\n├── background_texture\n├── embodiments\n│ ├── embodiment_1\n│ │ ├── config.yml\n│ │ └── ...\n│ └── ...\n├── objects\n└── ...\n## 5. Manual Installation (Only when step 3 failed) ¶\nInstall requirements\npip\ninstall\n-r\nrequirements.txt\nInstall pytorch3d\npip\ninstall\n\"git+https://github.com/facebookresearch/pytorch3d.git@stable\"\nInstall CuRobo\ncd envs\ngit clone https://github.com/NVlabs/curobo.git\ncd curobo\npip install -e . --no-build-isolation\ncd ../..\nAdjust code in\nmplib\n(\nImportant\n)\nYou can use\npip show mplib\nto find where the\nmplib\ninstalled.\nRemove\nor collide\n# mplib.planner (mplib/planner.py) line 807\n# remove `or collide`\nif\nnp\n.\nlinalg\n.\nnorm\n(\ndelta_twist\n)\n<\n1e-4\nor\ncollide\nor\nnot\nwithin_joint_limit\n:\nreturn\n{\n\"status\"\n:\n\"screw plan failed\"\n}\n=>\nif\nnp\n.\nlinalg\n.\nnorm\n(\ndelta_twist\n)\n<\n1e-4\nor\nnot\nwithin_joint_limit\n:\nreturn\n{\n\"status\"\n:\n\"screw plan failed\"\n}"}
{"id": "page-9", "contents": "Collect Data - RoboTwin 2.0 Offical Document\nSkip to content\n# Collect Data ¶\nWe provide over 100,000 pre-collected trajectories as part of the open-source release\nRoboTwin Dataset\n. However, we strongly recommend users to perform data collection themselves due to the high configurability and diversity of task and embodiment setups.\nRunning the following command will first search for a random seed for the target collection quantity, and then replay the seed to collect data.\nbash collect_data.sh ${task_name} ${task_config} ${gpu_id}\n# Example: bash collect_data.sh beat_block_hammer demo_randomized 0\nAfter data collection is completed, the collected data will be stored under\ndata/${task_name}/${task_config}\n.\nAn episode's data will be stored in one HDF5 file. Specifically, the images will be stored as bit streams. If you want to recover the image, you can use the following code:\nimage = cv2.imdecode(np.frombuffer(image_bit, np.uint8), cv2.IMREAD_COLOR)\nEach trajectory's observation and action data are saved in\nHDF5 format\nin the\ndata\ndirectory.\nThe corresponding\nlanguage instructions\nfor each trajectory are stored in the\ninstructions\ndirectory.\nHead camera videos\nof each trajectory can be found in the\nvideo\ndirectory.\nThe\n_traj_data\n,\n.cache\n,\nscene_info.json\n, and\nseed.txt\nfiles are auxiliary outputs generated during the data collection process.\nAll available\ntask_name\noptions can be found in the\ndocumentation\n. The\ngpu_id\nparameter specifies which GPU to use and should be set to an integer in the range\n0\nto\nN-1\n, where\nN\nis the number of GPUs available on your system.\nOur data synthesizer enables automated data collection by executing the task scripts in the\nenvs\ndirectory, in combination with the\ncurobo\nrobot planner. Specifically, data collection is configured through a task-specific configuration file (see the tutorial in\n./configurations.md\n), which defines parameters such as the target embodiment, domain randomization settings, and the number of data samples to collect.\nThe success rate of data generation for each embodiment across all tasks can be found at:\nhttps://robotwin-platform.github.io/doc/tasks/index.html\n. Due to the structural limitations of different robotic arms, not all embodiments are capable of completing every task.\nOur pipeline first explores a set of random seeds (\nseed.txt\n) to identify trajectories that can yield successful data collection. It then records fine-grained action trajectories (\n_traj_data\n) accordingly. Collected videos are available in the\nvideos\ndirectory.\nThe entire process is fully automated—just run a single command to get started.\n⚠️ The\nmissing pytorch3d\nwarning can be ignored if 3D data is not required."}
{"id": "page-10", "contents": "Domain Randomization - RoboTwin 2.0 Offical Document\nSkip to content\n# Domain Randomization ¶\nRoboTwin’s domain randomization primarily focuses on scene clutter, random lighting, over 12,000 tabletop textures, randomized tabletop heights, and camera viewpoint perturbations. The corresponding configuration options can be found at: 👉\nRoboTwin 2.0 Document (Usage: Configurations)"}
{"id": "page-11", "contents": "Configurations - RoboTwin 2.0 Offical Document\nSkip to content\n# Configuration Tutorial ¶\nAll configuration files are stored in the\ntask_config\nfolder and follow the standard YAML format.\nYou can run\nbash task_config/create_task_config.sh ${task_config_name}\nto create new task configuration.\n## 1. ✅ Minimal Example ¶\nAn episode's data will be stored in one HDF5 file. Specifically, the images will be stored as bit streams. If you want to recover the image, you can use the following code:\nimage = cv2.imdecode(np.frombuffer(image_bit, np.uint8), cv2.IMREAD_COLOR)\nBelow is a minimal configuration file to start a typical data collection session:\nrender_freq\n:\n0\nepisode_num\n:\n50\nuse_seed\n:\nfalse\nsave_freq\n:\n15\nembodiment\n:\n-\naloha-agilex\nlanguage_num\n:\n100\ndomain_randomization\n:\nrandom_background\n:\ntrue\ncluttered_table\n:\ntrue\nclean_background_rate\n:\n0.02\nrandom_head_camera_dis\n:\n0\nrandom_table_height\n:\n0.03\nrandom_light\n:\ntrue\ncrazy_random_light_rate\n:\n0.02\ncamera\n:\nhead_camera_type\n:\nD435\nwrist_camera_type\n:\nD435\ncollect_head_camera\n:\ntrue\ncollect_wrist_camera\n:\ntrue\ndata_type\n:\nrgb\n:\ntrue\nthird_view\n:\nfalse\ndepth\n:\nfalse\npointcloud\n:\nfalse\nobserver\n:\nfalse\nendpose\n:\ntrue\nqpos\n:\ntrue\nmesh_segmentation\n:\nfalse\nactor_segmentation\n:\nfalse\npcd_down_sample_num\n:\n1024\npcd_crop\n:\ntrue\nsave_path\n:\n./data\nclear_cache_freq\n:\n5\ncollect_data\n:\ntrue\neval_video_log\n:\ntrue\n## 2. 🔧 Configuration Breakdown ¶\n### 2.1 🎯 Task & Embodiment Settings ¶\nField\nType\nRequired\nDescription\nembodiment\nlist\n✅\nList of robot embodiment(s). For a dual-arm robot, use\n[name]\n, e.g.,\n[aloha-agilex]\n; to combine two single-arm robots, use\n[left, right, interval]\n, e.g.,\nembodiment: [piper, franka-panda, 0.6]\n,\nembodiment: [franka-panda, franka-panda, 0.8]\n. The\ninterval\nspecifies the distance between arms (typically 0.6–0.8 meters). Available Embodiment:\nur5-wsg\n,\nARX-X5\n,\nfranka-panda\n,\npiper\n,\naloha-agilex\n(dual-arm)\nuse_seed\nbool\n✅\nWhether to use a predefined seed list from\ndata/${task_name}/${task_config}/seed.txt\n. If\nfalse\n, the system will automatically explore viable seeds.\nepisode_num\nint\n✅\nNumber of\nsuccessful episodes\nto collect.\nlanguage_num\nint\noptional\nIf using language-conditioned task planning, sets the number of language descriptions to sample for each task.\n### 2.2 🧠 Domain Randomization ¶\nConfigure task variation for better generalization.\ndomain_randomization\n:\nrandom_background\n:\ntrue\ncluttered_table\n:\ntrue\nclean_background_rate\n:\n0.02\nrandom_head_camera_dis\n:\n0\nrandom_table_height\n:\n0.03\nrandom_light\n:\ntrue\ncrazy_random_light_rate\n:\n0.02\nrandom_embodiment\n:\nfalse\nField\nType\nDescription\nrandom_background\nbool\nEnable random textures for the table and background.\ncluttered_table\nbool\nAdd distractor objects to the table to simulate a cluttered environment.\nclean_background_rate\nfloat\nRatio of clean backgrounds (e.g.,\n0.02\n= 2%). Only effective if\nrandom_background\nis\ntrue\n.\nrandom_head_camera_dis\nfloat\nRandom displacement applied to the head camera position (in meters).\nrandom_table_height\nfloat\nRandom variation in the table height (in meters).\nrandom_light\nbool\nEnable randomized lighting during simulation.\ncrazy_random_light_rate\nfloat\nProbability of applying extreme lighting. Only effective if\nrandom_light\nis\ntrue\n.\nrandom_embodiment\nbool\nEnable embodiment randomization (experimental, currently not fully supported).\n### 2.3 📷 Camera Configuration ¶\ncamera\n:\nhead_camera_type\n:\nD435\nwrist_camera_type\n:\nD435\ncollect_head_camera\n:\ntrue\ncollect_wrist_camera\n:\ntrue\nField\nType\nDescription\nhead_camera_type\nstr\nCamera used for global observation. Options: see\ntask_config/_camera_config.yml\n.\nwrist_camera_type\nstr\nCamera used for close-up view.\ncollect_head_camera\nbool\nWhether to collect head-view data.\ncollect_wrist_camera\nbool\nWhether to collect wrist-view data.\n### 2.4 📦 Data Collection Settings ¶\nField\nType\nDescription\ncollect_data\nbool\nEnable actual data saving.\nsave_freq\nint\nSave every N steps. Per-step indicates 0.004s in the real world.\nsave_path\nstr\nDirectory to save data. Default:\n./data\n.\nclear_cache_freq\nint\nControls the frequency (in episodes) at which the Sapien scene cache is cleared. This helps manage GPU memory usage, especially when domain randomization is enabled and many diverse assets accumulate in memory. A smaller value (e.g., 1) increases clearing frequency but incurs additional time cost.\neval_video_log\nbool\nSave evaluation videos for replay.\n### 2.5 💾 Data Type ¶\nSpecify which data to collect in each episode:\ndata_type\n:\nrgb\n:\ntrue\nthird_view\n:\nfalse\ndepth\n:\nfalse\npointcloud\n:\nfalse\nobserver\n:\nfalse\nendpose\n:\nfalse\nqpos\n:\ntrue\nmesh_segmentation\n:\nfalse\nactor_segmentation\n:\nfalse\nType\nDescription\nrgb\nRGB image from multiple views.\nthird_view\nThird-person video.\ndepth\nDepth images from cameras (mm).\npointcloud\nMerged point cloud of the scene.\nobserver\nObserver-view RGB frame.\nendpose\nEnd-effector pose in the world coordinate frame and gripper opening ratio.\nqpos\nRobot joint angles.\nmesh_segmentation\nPer-object segmentation from mesh.\nactor_segmentation\nPer-actor segmentation from RGB.\n##### 2.5.1 Note ¶\nendpose\nwill get an dict containing\nleft_endpose\n,\nleft_gripper\n,\nright_endpose\nand\nright_gripper\n. The\nleft_endpose\nand\nright_endpose\nare list of 7 elements represent the position in world and orientation of the end-effectors, following the order\nx, y, z, qw, qx, qy, qz\n. And the\nleft_gripper\nand\nright_gripper\nare float numbers, which repersent the opening ratio of the gripper, ranging from 0 to 1. The rotation of end-effector is as the image below: for all embodiments, the end-effector rotation is consistent, with the x-axis pointing across the gripper and the z-axis pointing across the camera.\n## ¶\n### 2.6 🔍 Point Cloud Settings ¶\nField\nType\nDescription\npcd_down_sample_num\nint\nFPS (Farthest Point Sampling) number; set\n0\nto keep all points.\npcd_crop\nbool\nWhether to crop out table/walls based on known transforms.\n### 2.7 🎥 Rendering ¶\nField\nType\nDescription\nrender_freq\nint\nRender visualization every N steps. Set to\n0\nto disable. For servers without display, recommend\n0\n. If you want to visualize the task, try to modify it to\n20\n(as example)\n## 3. 📌 Notes ¶\nAll task names must correspond to files in\nenv/<task_name>.py\n.\nFor available embodiments and cameras, refer to:\ntask_config/_embodiment_config.yml\ntask_config/_camera_config.yml\nThe system supports both dual-arm and single-arm setups.\nSeeds, if used, are located in\ntask_config/seeds/\n."}
{"id": "page-12", "contents": "Control Robot - RoboTwin 2.0 Offical Document\nSkip to content\n# Control Robot ¶\nThe\ntake_action\nfunction in\n_base_task\nis used to control actions during task execution. It accepts two parameters:\naction\nand\naction_type\n.\n## 1. Supported Action Types ¶\nThe parameter\naction_type\nsupports two modes:\nqpos\n(Joint Position Control) —\ndefault\nee\n(End-Effector Pose Control)\nDepending on the selected mode, the format and dimension of the input\naction\nwill differ.\n### 1.1 qpos Mode (Joint Position Control) ¶\nIn\nqpos\nmode, the\naction\nis defined as:\n[left_arm_joints + left_gripper + right_arm_joints + right_gripper]\nThe specific dimension of the\naction\ndepends on the robotic arm configuration.\nThe system will\nautomatically adjust\nthe input dimensions during deployment to match the specific robot configuration.\n### 1.2 ee Mode (End-Effector Pose Control) ¶\nIn\nee\nmode, the\naction\nis defined as:\n[left_end_effector_pose (xyz + quaternion) + left_gripper + right_end_effector_pose + right_gripper]\nThe dimension is\nfixed\n, regardless of the robot configuration.\n## 2. Deployment Example ¶\nYou can find a demonstration of usage in:\npolicy/Your_Policy/deploy_policy.py\nThis file provides a sample implementation to help you understand how to use the\ntake_action\nfunction with different\naction_type\nsettings during deployment."}
{"id": "page-13", "contents": "Deploy Your Policy - RoboTwin 2.0 Offical Document\nSkip to content\n# 🚀 Deploy Your Policy ¶\nTo deploy and evaluate your policy, you need to\nmodify the following three files\n:\neval.sh\n:\neval.sh demo\ndeploy_policy.yml\n:\ndeploy_policy.yml demo\ndeploy_policy.py\n:\ndeploy_policy.py demo\nIn\ndeploy_policy.py\n, the following components are defined:\nget_model\nfor loading the policy model,\nencode_obs\nfor observation processing (modification may not be necessary), and\nget_action\nalong with the control loop that handles observation acquisition and action execution.\nThe\ndeploy_policy.yml\nfile specifies the input parameters, which are eventually passed into the\nget_model\nfunction as\nusr_args\nto assist in locating, defining, and loading your model.\nIn\neval.sh\n, the parameters specified after\noverrides\ncan be used to overwrite those in\ndeploy_policy.yml\n, allowing you to specify different settings without manually modifying the YAML file each time.\n# policy/Your_Policy/deploy_policy.py\n# import packages and module here\ndef encode_obs(observation): # Post-Process Observation\nobs = observation\n# ...\nreturn obs\ndef get_model(usr_args): # from deploy_policy.yml and eval.sh (overrides)\nYour_Model = None\n# ...\nreturn Your_Model # return your policy model\ndef eval(TASK_ENV, model, observation):\n\"\"\"\nAll the function interfaces below are just examples\nYou can modify them according to your implementation\nBut we strongly recommend keeping the code logic unchanged\n\"\"\"\nobs = encode_obs(observation) # Post-Process Observation\ninstruction = TASK_ENV.get_instruction()\nif len(\nmodel.obs_cache\n) == 0: # Force an update of the observation at the first frame to avoid an empty observation window, `obs_cache` here can be modified\nmodel.update_obs(obs)\nactions = model.get_action() # Get Action according to observation chunk\nfor action in actions: # Execute each step of the action\n# see for https://robotwin-platform.github.io/doc/control-robot.md more details\nTASK_ENV.take_action(action, action_type='qpos') # joint control: [left_arm_joints + left_gripper + right_arm_joints + right_gripper]\n# TASK_ENV.take_action(action, action_type='ee') # endpose control: [left_end_effector_pose (xyz + quaternion) + left_gripper + right_end_effector_pose + right_gripper]\n# TASK_ENV.take_action(action, action_type='delta_ee') # delta endpose control: [left_end_effector_delta (xyz + quaternion) + left_gripper + right_end_effector_delta + right_gripper]\nobservation = TASK_ENV.get_obs()\nobs = encode_obs(observation)\nmodel.update_obs(obs) # Update Observation, `update_obs` here can be modified\ndef reset_model(model):\n# Clean the model cache at the beginning of every evaluation episode, such as the observation window\npass\n## 1. 🔧 deploy_policy.yml ¶\nYou are free to\nadd any parameters\nneeded in\ndeploy_policy.yml\nto specify your model setup (e.g., checkpoint path, model type, architecture details). The entire YAML content will be passed to\ndeploy_policy.py\nas\nusr_args\n, which will be available in the\nget_model()\nfunction.\n## 2. 🖥️ eval.sh ¶\nUpdate the script to pass additional arguments to override default values in\ndeploy_policy.yml\n.\n#!/bin/bash\npolicy_name\n=\nYour_Policy\ntask_name\n=\n${\n1\n}\ntask_config\n=\n${\n2\n}\nckpt_setting\n=\n${\n3\n}\nseed\n=\n${\n4\n}\ngpu_id\n=\n${\n5\n}\n# [TODO] Add your custom command-line arguments here\nexport\nCUDA_VISIBLE_DEVICES\n=\n${\ngpu_id\n}\necho\n-e\n\"\\033[33mgpu id (to use):\n${\ngpu_id\n}\n\\033[0m\"\ncd\n../..\n# move to project root\npython\nscript/eval_policy.py\n--config\npolicy/\n$policy_name\n/deploy_policy.yml\n\\\n--overrides\n\\\n--task_name\n${\ntask_name\n}\n\\\n--task_config\n${\ntask_config\n}\n\\\n--ckpt_setting\n${\nckpt_setting\n}\n\\\n--seed\n${\nseed\n}\n\\\n--policy_name\n${\npolicy_name\n}\n# [TODO] Add your custom arguments here\n## 3. 🧠 deploy_policy.py ¶\nYou need to implement the following methods in\ndeploy_policy.py\n:\n### 3.1 encode_obs(obs: dict) -> dict ¶\nOptional. This function is used to preprocess the raw environment observation (e.g., color channel normalization, reshaping, etc.). If not needed, it can be left unchanged.\n### 3.2 get_model(usr_args: dict) -> Any ¶\nRequired. This function receives the full configuration from\ndeploy_policy.yml\nvia\nusr_args\nand must return the initialized model. You can define your own loading logic here, including parsing checkpoints and network parameters.\n### 3.3 eval(env, model, observation, instruction) -> Any ¶\nRequired. The main evaluation loop. Given the current environment instance, model, and observation (as a dictionary), and a natural language\ninstruction\n(string), this function must compute the next action and execute it in the environment.\n### 3.4 update_obs(obs: dict) -> None ¶\nOptional. Used to update any internal state of the model or observation buffer. Useful if your model requires a history of frames or a memory-based context.\n### 3.5 get_action(model, obs: dict) -> Any ¶\nOptional. Given a model and current observation, return the action to be executed. This is useful if action computation is separated from the evaluation loop.\n### 3.6 reset_model() -> None ¶\nOptional but\nrecommended\n. This function is called before the evaluation of\neach episode\n, allowing you to reset model states such as recurrent memory, history buffers, or context encodings.\n## 4. ✔️ Run eval.sh ¶\nbash eval.sh ...(input parameters you define)\n## 5. 📌 Notes ¶\nThe variable\ninstruction\nis a string containing the language command describing the task. You can choose how (or whether) to use it.\nYour policy should be compatible with the input/output format expected by the simulator."}
{"id": "page-14", "contents": "ACT - RoboTwin 2.0 Offical Document\nSkip to content\n# ACT (Action Chunking Transformer) ¶\n## 1. Install ¶\ncd policy/ACT\npip install pyquaternion pyyaml rospkg pexpect mujoco==2.3.7 dm_control==1.0.14 opencv-python matplotlib einops packaging h5py ipython\ncd detr && pip install -e . && cd ..\n## 2. Prepare Training Data ¶\nThis step performs data preprocessing, converting the original\nRoboTwin 2.0\ndata into the format required for ACT training. The\nexpert_data_num\nparameter specifies the number of trajectory pairs to be used as training data.\nbash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data.sh beat_block_hammer demo_randomized 50\n## 3. Train Policy ¶\nThis step launches the training process. By default, the model is trained for\n6,000 steps\n.\nbash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${gpu_id}\n# bash train.sh beat_block_hammer demo_randomized 50 0 0\n## 4. Eval Policy ¶\nThe\ntask_config\nfield refers to the\nevaluation environment configuration\n, while the\nckpt_setting\nfield refers to the\ntraining data configuration\nused during policy learning.\nbash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized demo_randomized 50 0 0\n# This command trains the policy using the `demo_randomized` setting ($ckpt_setting)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean demo_randomized 50 0 0\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root."}
{"id": "page-15", "contents": "DP - RoboTwin 2.0 Offical Document\nSkip to content\n# DP (Diffusion Policy) ¶\n## 1. Install ¶\ncd policy/DP\npip install zarr==2.12.0 wandb ipdb gpustat dm_control omegaconf hydra-core==1.2.0 dill==0.3.5.1 einops==0.4.1 diffusers==0.11.1 numba==0.56.4 moviepy imageio av matplotlib termcolor sympy\npip install -e .\n## 2. Prepare Training Data ¶\nThis step performs data preprocessing, converting the original\nRoboTwin 2.0\ndata into the\nZarr format\nrequired for DP training. The\nexpert_data_num\nparameter specifies the number of trajectory pairs to be used as training data.\nbash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data.sh beat_block_hammer demo_randomized 50\n## 3. Train Policy ¶\nThis step launches the training process. By default, the model is trained for\n600 steps\n. The\naction_dim\nparameter defines the dimensionality of the robot’s action space — for example, it is\n14\nfor the\naloha-agilex\nembodiment.\nbash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${action_dim} ${gpu_id}\n# bash train.sh beat_block_hammer demo_randomized 50 0 14 0\n# For `aloha-agilex` embodiment, the action_dim is 14\n## 4. Eval Policy ¶\nThe\ntask_config\nfield refers to the\nevaluation environment configuration\n, while the\nckpt_setting\nfield refers to the\ntraining data configuration\nused during policy learning.\nbash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized demo_randomized 50 0 0\n# This command trains the policy using the `demo_randomized` setting ($ckpt_setting)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean demo_randomized 50 0 0\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root."}
{"id": "page-16", "contents": "DP3 - RoboTwin 2.0 Offical Document\nSkip to content\n# DP3 (3D Diffusion Policy) ¶\nSince\nDP3\nis a 3D policy that requires point cloud input, please make sure to set\ndata_type/pointcloud\nto\ntrue\nduring data collection.\n## 1. Install ¶\ncd policy/DP3/3D-Diffusion-Policy && pip install -e . && cd ..\npip install zarr==2.12.0 wandb ipdb gpustat dm_control omegaconf hydra-core==1.2.0 dill==0.3.5.1 einops==0.4.1 diffusers==0.11.1 numba==0.56.4 moviepy imageio av matplotlib termcolor\n## 2. Prepare Training Data ¶\nIf you meet\nZeroDivisionError: division by zero\n: Since\nDP3\nis a 3D policy that requires point cloud input, please make sure to set\ndata_type/pointcloud\nto\ntrue\nduring data collection.\nThis step performs data preprocessing, converting the original\nRoboTwin 2.0\ndata into the\nZarr format\nrequired for DP3 training. The\nexpert_data_num\nparameter specifies the number of trajectory pairs to be used as training data.\nbash process_data.sh ${task_name} ${task_config} ${expert_data_num}\n# bash process_data.sh beat_block_hammer demo_randomized 50\n## 3. Train Policy ¶\nThis step launches the training process. By default, the model is trained for\n3,000 steps\n.\nbash train.sh ${task_name} ${task_config} ${expert_data_num} ${seed} ${gpu_id}\n# bash train.sh beat_block_hammer demo_randomized 50 0 0\n## 4. Eval Policy ¶\nThe\ntask_config\nfield refers to the\nevaluation environment configuration\n, while the\nckpt_setting\nfield refers to the\ntraining data configuration\nused during policy learning.\nbash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized demo_randomized 50 0 0\n# This command trains the policy using the `demo_randomized` setting ($ckpt_setting)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean demo_randomized 50 0 0\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root."}
{"id": "page-17", "contents": "RDT - RoboTwin 2.0 Offical Document\nSkip to content\n# RDT ¶\n## 1. Environment Setup ¶\nThe conda environment for RDT with RoboTwin is identical to the official RDT environment. Please follow the (\nRDT official documentation\n) to install the environment and directly overwrite the RoboTwin virtual environment in\nINSTALLATION.md\n.\n# Make sure python version == 3.10\nconda\nactivate\nRoboTwin\n# Install pytorch\n# Look up https://pytorch.org/get-started/previous-versions/ with your cuda version for a correct command\npip\ninstall\ntorch\n==\n2\n.1.0\ntorchvision\n==\n0\n.16.0\n--index-url\nhttps://download.pytorch.org/whl/cu121\n# Install packaging\npip\ninstall\npackaging\n==\n24\n.0\npip\ninstall\nninja\n# Verify Ninja --> should return exit code \"0\"\nninja\n--version\n;\necho\n$?\n# Install flash-attn\npip\ninstall\nflash-attn\n==\n2\n.7.2.post1\n--no-build-isolation\n# Install other prequisites\npip\ninstall\n-r\nrequirements.txt\n# If you are using a PyPI mirror, you may encounter issues when downloading tfds-nightly and tensorflow.\n# Please use the official source to download these packages.\n# pip install tfds-nightly==4.9.4.dev202402070044 -i https://pypi.org/simple\n# pip install tensorflow==2.15.0.post1 -i https://pypi.org/simple\n## 2. Download Model ¶\n# In the ROOT directory\ncd\npolicy\nmkdir\nweights\ncd\nweights\nmkdir\nRDT\n&&\ncd\nRDT\n# Download the models used by RDT\nhuggingface-cli\ndownload\ngoogle/t5-v1_1-xxl\n--local-dir\nt5-v1_1-xxl\nhuggingface-cli\ndownload\ngoogle/siglip-so400m-patch14-384\n--local-dir\nsiglip-so400m-patch14-384\nhuggingface-cli\ndownload\nrobotics-diffusion-transformer/rdt-1b\n--local-dir\nrdt-1b\n## 3. Collect RoboTwin Data ¶\nSee\nRoboTwin Tutorial (Usage Section)\nfor more details.\n## 4. Generate HDF5 Data ¶\nHDF5 is the data format required for RDT training.\nFirst, create the\nprocessed_data\nand\ntraining_data\nfolders in the\npolicy/RDT\ndirectory:\nmkdir\nprocessed_data\n&&\nmkdir\ntraining_data\nThen, run the following in the\nRDT/\nroot directory:\nbash\nprocess_data_rdt.sh\n${\ntask_name\n}\n${\ntask_config\n}\n${\nexpert_data_num\n}\n${\ngpu_id\n}\nIf success, you will find the\n${task_name}-${task_config}-${expert_data_num}\nfolder under\npolicy/RDT/processed_data\n.\n## 5. Generate Configuration File ¶\nA\n$model_name\nmanages the training of a model, including the training data and training configuration.\ncd\npolicy/RDT\nbash\ngenerate.sh\n${\nmodel_name\n}\n# bash generate.sh RDT_demo_randomized\nThis will create a folder named\n\\${model_name}\nunder training_data and a configuration file\n\\${model_name}.yml\nunder model_config.\n### 5.1 Prepare Data ¶\nCopy all the data you wish to use for training from\nprocessed_data\ninto\ntraining_data/${model_name}\n. If you have multiple tasks with different data, simply copy them in the same way.\nExample folder structure:\ntraining_data/${model_name}\n├── ${task_1}\n│ ├── episode_0\n| | |── episode_0.hdf5\n| | |-- instructions\n| │ │ ├── lang_embed_0.pt\n| │ │ ├── ...\n├── ${task_2}\n│ ├── ...\n├── ...\n### 5.2 Modify Training Config ¶\nIn\nmodel_config/${model_name}.yml\n, you need to manually set the GPU to be used (modify\ncuda_visible_device\n). For a single GPU, try format like\n0\nto set GPU 0. For multi-GPU usage, try format like\n0,1,4\n. You can flexibly modify other parameters.\n## 6. Finetune model ¶\nOnce the training parameters are set, you can start training with:\nbash\nfinetune.sh\n${\nmodel_name\n}\n# bash finetune.sh RDT_demo_randomized\nNote!\nIf you fine-tune the model using a single GPU, DeepSpeed will not save\npytorch_model/mp_rank_00_model_states.pt\n. If you wish to continue training based on the results of a single-GPU trained model, please set\npretrained_model_name_or_path\nto something like\n./checkpoints/${model_name}/checkpoint-${ckpt_id}\n.\nThis will use the pretrain pipeline to import the model, which is the same import structure as the default\n../weights/RDT/rdt-1b\n.\n## 7. Eval on RoboTwin ¶\nThe\ntask_config\nfield refers to the\nevaluation environment configuration\n, while the\nmodel_name\nfield refers to the\ntraining data configuration\nused during policy learning.\nbash\neval.sh\n${\ntask_name\n}\n${\ntask_config\n}\n${\nmodel_name\n}\n${\ncheckpoint_id\n}\n${\nseed\n}\n${\ngpu_id\n}\n# bash eval.sh beat_block_hammer demo_randomized RDT_demo_randomized 10000 0 0\n# This command trains the policy using the `RDT_demo_randomized` setting ($model_name)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n#\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean RDT_demo_randomized 10000 0 0\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root."}
{"id": "page-18", "contents": "Pi0 - RoboTwin 2.0 Offical Document\nSkip to content\n# OpenPI ¶\n## 1. Environment Setup ¶\nWe use\nuv\nto manage Python dependencies,you can add uv your conda environment.\nconda\nactivate\nRoboTwin\n# Install uv\npip\ninstall\nuv\nOnce uv is installed, run the following commands to set up the environment:\ncd\npolicy/pi0\n# Install prequisites in uv environment\nGIT_LFS_SKIP_SMUDGE\n=\n1\nuv\nsync\nIf you want to eval pi0 policy in RoboTwin，you are required to install curobo in your uv environment：\nconda\ndeactivate\nsource\n.venv/bin/activate\n# At this point, you should be in the (openpi) environment\ncd\n../../envs\ngit\nclone\nhttps://github.com/NVlabs/curobo.git\ncd\ncurobo\npip\ninstall\n-e\n.\n--no-build-isolation\ncd\n../../policy/pi0/\nbash\n## 2. Generate RoboTwin Data ¶\nSee\nRoboTwin Tutorial (Usage Section)\nfor more details.\n## 3. Generate openpi Data ¶\nFirst, create the\nprocessed_data\nand\ntraining_data\nfolders in the\npolicy/pi0\ndirectory:\nmkdir processed_data && mkdir training_data\nThen, convert RoboTwin data to HDF5 data type.\nbash\nprocess_data_pi0.sh\n${\ntask_name\n}\n${\ntask_config\n}\n${\nexpert_data_num\n}\n# bash process_data_pi0.sh beat_block_hammer demo_randomized 50\nIf success, you will find the\n${task_name}-${task_config}-${expert_data_num}\nfolder under\npolicy/pi0/processed_data\n.\nExample folder structure:\nprocessed_data/\n├──\n${\ntask_name\n}\n-\n${\ntask_config\n}\n-\n${\nexpert_data_num\n}\n|\n|\n├──episode_0\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_0.hdf5\n|\n|\n├──\nepisode_1\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_1.hdf5\n|\n|\n├──\n...\nCopy all the data you wish to use for training from\nprocessed_data\ninto\ntraining_data/${model_name}\n. If you have multiple tasks with different data, simply copy them in the same way.please Place the corresponding task folders according to the example below.\n#multi-task dataset example\ntraining_data/\n├──\n${\nmodel_name\n}\n|\n├──\n${\ntask_0\n}\n|\n|\n├──episode_0\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_0.hdf5\n|\n|\n├──\nepisode_1\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_1.hdf5\n|\n|\n├──\n...\n|\n├──\n${\ntask_1\n}\n|\n|\n├──episode_0\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_0.hdf5\n|\n|\n├──\nepisode_1\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_1.hdf5\n|\n|\n├──\n...\n#sigle task example\ntraining_data/\n├──\ndemo_randomized\n|\n├──beat_block_hammer-demo_randomized-50\n|\n|\n├──episode_0\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_0.hdf5\n|\n|\n├──\nepisode_1\n|\n|\n|\n├──\ninstructions.json\n|\n|\n|\n├──\nepisode_1.hdf5\n|\n|\n├──\n...\nBefore generating the LerobotDataset format data for pi0,please make sure you have enough disk space under the\n~/.cache\n.This is because generating the\nlerobotdataset\nwill require a large amount of space.And the datasets will be writed into\n$XDG_CACHE_HOME\n,which default path is\n~/.cache\n.If you don't have enough disk space under the\n~/.cache\npath, please use the following command to set a different cache directory with sufficient space:\nexport\nXDG_CACHE_HOME\n=\n/path/to/your/cache\nNow, we can directly generate the LerobotDataset format data for pi0\n# hdf5_path: The path to the generated HDF5 data (e.g., ./training_data/${model_name}/)\n# repo_id: The name of the dataset (e.g., my_repo)\nbash\ngenerate.sh\n${\nhdf5_path\n}\n${\nrepo_id\n}\n#bash generate.sh ./training_data/demo_randomized/ demo_randomized_repo\nLerobotDataset format data will be writed into\n${XDG_CACHE_HOME}/huggingface/lerobot/${repo_id}\n## 4. Write the Corresponding train_config ¶\nIn\nsrc/openpi/training/config.py\n, there is a dictionary called\n_CONFIGS\n. You can modify 4 pre-configured PI0 configurations I’ve written:\npi0_base_aloha_robotwin_lora\npi0_fast_aloha_robotwin_lora\npi0_base_aloha_robotwin_full\npi0_fast_aloha_robotwin_full\nYou only need to write\nrepo_id\non your datasets.(e.g.,\nrepo_id=demo_randomized_repo\n) If you want to change the\nname\nin\nTrainConfig\n, please include\nfast\nif you choose\npi_fast_base\nmodel. If your do not have enough gpu memory, you can set\nfsdp_devices\n, refer to\nconfig.py\nline\nsrc/openpi/training/config.py\nline 352.\n## 5. 5. Finetune model ¶\n# compute norm_stat for dataset\nuv\nrun\nscripts/compute_norm_stats.py\n--config-name\n${\ntrain_config_name\n}\n# uv run scripts/compute_norm_stats.py --config-name pi0_base_aloha_robotwin_full\n# train_config_name: The name corresponding to the config in _CONFIGS, such as pi0_base_aloha_robotwin_full\n# model_name: You can choose any name for your model\n# gpu_use: if not using multi gpu,set to gpu_id like 0;else set like 0,1,2,3\nbash\nfinetune.sh\n${\ntrain_config_name\n}\n${\nmodel_name\n}\n${\ngpu_use\n}\n#bash finetune.sh pi0_base_aloha_robotwin_full demo_randomized 0,1,2,3\nTraining mode\nMemory Required\nExample GPU\nFine-Tuning (LoRA)\n> 46 GB\nA6000(48G)\nFine-Tuning (Full)\n> 100 GB\n2*A100 (80GB) / 2*H100\nIf your GPU memory is insufficient, please set the\nfsdp_devices\nparameter according to the following GPU memory reference, or reduce the\nbatch_size\nparameter. Or you can try setting\nXLA_PYTHON_CLIENT_PREALLOCATE=false\nin\nfinetune.sh\n, it will cost lower gpu memory, but make training speed slower.\nThe default\nbatch_size\nis 32 in the table below.\nGPU memory\nModel type\nGPU num\nfsdp_devices\nExample GPU\n24G\nlora\n2\n2\n4090(24G)\n40G\nlora\n2\n2\nA100(40G)\n48G\nlora\n1\n1\nA6000(48G)\n40G\nfull\n4\n4\nA100(40G)\n80G\nfull\n2\n2\nA100(80G)\n## 6. Eval on RoboTwin ¶\nCheckpoints will be saved in policy/pi0/checkpoints/\n\\({train_config_name}/\\)\n}/${checkpoint_id\nYou can modify the\ndeploy_policy.yml\nfile to change the\ncheckpoint_id\nyou want to evaluate.\n# ckpt_path like: policy/pi0/checkpoints/pi0_base_aloha_robotwin_full/demo_randomized/30000\nbash\neval.sh\n${\ntask_name\n}\n${\ntask_config\n}\n${\ntrain_config_name\n}\n${\nmodel_name\n}\n${\nseed\n}\n${\ngpu_id\n}\n# bash eval.sh beat_block_hammer demo_randomized pi0_base_aloha_robotwin_full demo_randomized 0 0\n# This command trains the policy using the `demo_randomized` setting ($model_name)\n# and evaluates it using the same `demo_randomized` setting ($task_config).\n# To evaluate a policy trained on the `demo_randomized` setting and tested on the `demo_clean` setting, run:\n# bash eval.sh beat_block_hammer demo_clean pi0_base_aloha_robotwin_full demo_randomized 0 0\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root."}
{"id": "page-19", "contents": "DexVLA - RoboTwin 2.0 Offical Document\nSkip to content\n# DexVLA (Vision-Language Model with Plug-In Diffusion Expert for Visuomotor Policy Learning) ¶\nContributed by Midea Group\n## 1. Install ¶\nTo guarantee clean isolation between training and evaluation environments for both DexVLA and TinyVLA, we provide two distinct, self-contained setups.The training and testing environment can be used for both DexVLA and TinyVLA.\nTraining Environment：\ncd\npolicy/DexVLA\nconda\nenv\ncreate\n-f\nTrain_Tiny_DexVLA_train.yml\nconda\nactivate\ndexvla-robo\ncd\npolicy_heads\npip\ninstall\n-e\n.\nEvaluation Environment:\nIf you already have RoboTwin 2.0 installed, activate this conda environment and add the evaluation dependencies:\nconda\nactivate\nyour_RoboTwin_env\npip\ninstall\n-r\nEval_Tiny_DexVLA_requirements.txt\n## 2. Prepare Training Data ¶\nThis step performs data preprocessing, converting the original RoboTwin 2.0 data into the format required for DexVLA training. The\nexpert_data_num\nparameter specifies the number of trajectory pairs to be used as training data.\npython\nprocess_data.py\n${\ntask_name\n}\n${\ntask_config\n}\n${\nexpert_data_num\n}\n# python process_data.py beat_block_hammer demo_randomized 50\nIf success, you will find the data in the\npolicy/Dexvla/data/sim_${task_name}/${setting}_${expert_data_num}\nfolder.\n## 3. Train Policy ¶\nThis step launches the training process.\n### 3.1 Download official Qwen2_VL weights ¶\nWe construct the VLM backbone by integrating Qwen2-VL-2B.You can download the official weights from this link:\nModel\nLink\nQwen2-VL (~2B)\nhuggingface\n❗❗\nAfter downloading the standard weights, you have to modify the official\nconfig.json\nfile in the folder. Please update the 'architectures' field from \"Qwen2VLForConditionalGenerationForVLA\" to \"DexVLA\", and change the 'model_type' field from \"qwen2_vla\" to \"dex_vla\".\n### 3.2 Download our pretrained ScaleDP-H weights ¶\nWe released our pretrained weights of ScaleDP-H which is trained after Stage1. Now you can download the weights and directly finetuning your data on Stage 2.\nModel\nLink\nScaleDP-H (~1B)\nhuggingface\nScaleDP-L (~400M)\nhuggingface\n### 3.3 Train\nThe training script are \"scripts/aloha/vla_stage2_train.sh\". And you need to change following parameters:\n1.\nOUTPUT\n: refers to the save directory for training, which must include the keyword \"qwen2\" (and optionally \"lora\"). If LoRA training is used, the name must include \"lora\" (e.g., \"qwen2_lora\").\n2.\nTASKNAME\n: refers to the tasks used for training, which should be corresponded to \"your_task_name\" in aloha_scripts/constant.py\n3.\nmnop\n: path to the pretrained VLM weights\n4.\nload_pretrain_dit\n: True\n5.\nDIT_PRETRAIN\n:Path to pretrained policy head (ScaleDP).\nOther hyperparameters like \"batch_size\", \"save_steps\" could be customized according to your computation resources.\nStart training by following commands:\nbash\n./scripts/aloha/vla_stage2_train.sh\n## 4. Eval Policy ¶\nYou need to modify the corresponding path in the\ndeploy_policy.yml\nfile: 1.\nmodel_path\n: Path to the trained model, in the OUTPUT path. 2.\nstate_path\n: Path to\ndataset_stats.pkl\n, in the OUTPUT path.\nThen execute:\nbash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized 0 50 0 0\n## 5. Citation ¶\nIf you find our works useful for your research and applications, please cite using these BibTeX:\n### 5.1 DexVLA ¶\n@article\n{\nwen2025dexvla\n,\ntitle\n=\n{DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control}\n,\nauthor\n=\n{Wen, Junjie and Zhu, Yichen and Li, Jinming and Tang, Zhibin and Shen, Chaomin and Feng, Feifei}\n,\njournal\n=\n{arXiv preprint arXiv:2502.05855}\n,\nyear\n=\n{2025}\n}\n### 5.2 DiffusionVLA ¶\n@article\n{\nwen2024diffusion\n,\ntitle\n=\n{Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression}\n,\nauthor\n=\n{Wen, Junjie and Zhu, Minjie and Zhu, Yichen and Tang, Zhibin and Li, Jinming and Zhou, Zhongyi and Li, Chengmeng and Liu, Xiaoyu and Peng, Yaxin and Shen, Chaomin and others}\n,\njournal\n=\n{arXiv preprint arXiv:2412.03293}\n,\nyear\n=\n{2024}\n}\n### 5.3 ScaleDP ¶\n@article\n{\nzhu2024scaling\n,\ntitle\n=\n{Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation}\n,\nauthor\n=\n{Zhu, Minjie and Zhu, Yichen and Li, Jinming and Wen, Junjie and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and others}\n,\njournal\n=\n{arXiv preprint arXiv:2409.14411}\n,\nyear\n=\n{2024}\n}"}
{"id": "page-20", "contents": "TinyVLA - RoboTwin 2.0 Offical Document\nSkip to content\n# Tiny-VLA (Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation) ¶\nContributed by Midea Group\n## 1. Install ¶\nTo guarantee clean isolation between training and evaluation environments for both DexVLA and TinyVLA, we provide two distinct, self-contained setups.The training and testing environment can be used for both DexVLA and TinyVLA.\nTraining Environment：\ncd\npolicy/TinyVLA\nconda\nenv\ncreate\n-f\nTrain_Tiny_DexVLA_train.yml\nconda\nactivate\ndexvla-robo\ncd\npolicy_heads\npip\ninstall\n-e\n.\nEvaluation Environment:\nIf you already have RoboTwin 2.0 installed, activate its conda environment and add the evaluation dependencies:\nconda\nactivate\nyour_RoboTwin_env\npip\ninstall\n-r\nEval_Tiny_DexVLA_requirements.txt\n## 2. Prepare Training Data ¶\nThis step performs data preprocessing, converting the original RoboTwin 2.0 data into the format required for TinyVLA training. The\nexpert_data_num\nparameter specifies the number of trajectory pairs to be used as training data.\npython\nprocess_data.py\n${\ntask_name\n}\n${\ntask_config\n}\n${\nexpert_data_num\n}\n# python process_data.py beat_block_hammer demo_randomized 50\nIf success, you will find the\nsim_${task_name}/${setting}_${expert_data_num}\nfolder under\npolicy/Tinyvla/data\n.\n## 3. Train Policy ¶\nThis step launches the training process. First, download the VLM model InternVL3-1B (\nhuggingface\n) to the path\n.../policy/TinyVLA/model_param/InternVL3-1B\n. Then modify the\nconfig.json\nfile in the folder as follows:\n{\n\"_name_or_path\": \".../robotiwin/policy/TinyVLA/vla/models/internvl\", # Modify this.\n\"architectures\": [\n\"TinyVLA\" # Change this.\n],\n# \"auto_map\":{...} # Delete this.\n...\n\"llm_config\": {}, # Don't Change.\n\"min_dynamic_patch\": 1,\n\"model_type\": \"tinyvla\", # Change this.\n...\n}\nThen add an task config item in\n.../policy/TinyVLA/aloha_scripts/constants.py\nTASK_CONFIGS\n=\n{\n...\n\"your_task\"\n:\n{\n'dataset_dir'\n:\n[\nDATA_DIR\n+\n\"/sim-your_task/aloha-agilex-1-m1_b1_l1_h0.03_c0_D435-100\"\n],\n'episode_len'\n:\n500\n,\n'camera_names'\n:\n[\n'cam_high'\n,\n'cam_left_wrist'\n,\n'cam_right_wrist'\n],\n\"sample_weights\"\n:\n[\n1\n,\n1\n]\n}\n}\nThen begin the training\nbash\n./scripts/franks/train_robotwin_aloha.sh\nConfigure the training by modifying the following items in the\ntrain_robotwin_aloha.sh\nfile.\nTASK=your_task # Set the Task\nROOT=.../robotiwin/policy/TinyVLA # Set Root Path\nmnop=.../robotiwin/policy/TinyVLA/model_param/InternVL3-1B/ # Set The Path of base VLM\n## 4. Eval Policy ¶\nYou need to modify the corresponding path in the\ndeploy_policy.yml\nfile: 1.\nmodel_path\n: Path to the trained model, in the OUTPUT path. 2.\nstate_path\n: Path to\ndataset_stats.pkl\n, in the OUTPUT path. 3.\nmodel_base\n: Path to InternVL3-1B.\nThen execute:\nbash eval.sh ${task_name} ${task_config} ${ckpt_setting} ${expert_data_num} ${seed} ${gpu_id}\n# bash eval.sh beat_block_hammer demo_randomized 0 50 0 0\n## 5. Citation ¶\nIf you find Tiny-VLA useful for your research and applications, please cite using this BibTeX:\n@inproceedings\n{\nwen2024tinyvla\n,\ntitle\n=\n{Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation}\n,\nauthor\n=\n{Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Liu, Ning and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and others}\n,\nbooktitle\n=\n{IEEE Robotics and Automation Letters (RA-L)}\n,\nyear\n=\n{2025}\n}"}
{"id": "page-21", "contents": "OpenVLA-oft - RoboTwin 2.0 Offical Document\nSkip to content\n# Openvla-oft ¶\n## 1. Environment Setup ¶\nThe conda environment for openvla-oft with RoboTwin is identical to the official openvla-oft environment for the ALOHA part. Please follow the (\nopenvla-oft official documentation\n) to install the environment and directly overwrite the RoboTwin environment.\nconda\nactivate\nRoboTwin\n# Install PyTorch\n# Use a command specific to your machine: https://pytorch.org/get-started/locally/\npip3\ninstall\ntorch\ntorchvision\ntorchaudio\n# Clone openvla-oft repo and pip install to download dependencies\ngit\nclone\nhttps://github.com/moojink/openvla-oft.git\ncd\nopenvla-oft\npip\ninstall\n-e\n.\n# Install Flash Attention 2 for training (https://github.com/Dao-AILab/flash-attention)\n# =>> If you run into difficulty, try `pip cache remove flash_attn` first\npip\ninstall\npackaging\nninja\nninja\n--version\n;\necho\n$?\n# Verify Ninja --> should return exit code \"0\"\npip\ninstall\n\"flash-attn==2.5.5\"\n--no-build-isolation\nNote!\nIf you encounter problems on diffusers, try\npip install diffusers==0.33.1\n## 2. Collect RoboTwin Data ¶\nSee\nRoboTwin Tutorial (Usage Section)\nfor more details.\n## 3. Generate RLDS Data ¶\nRLDS dataset is the data format required for Openvla-oft training.\nuse RoboTwin data generation mechanism to generate data.\nThen convert the raw data to the aloha format that openvla-oft accepts:\nbash preproces_aloha.sh\nThen transform the data to tfds form and register the tfds form dataset in your device: e.g.:\npython -m datasets.move_can_pot_builder\nAfter converting to RLDS, register the dataset (which, for example, would be called\naloha_move_can_pot_builder\n) with our dataloader by adding an entry for it in\nconfigs.py\n(\nhere\n),\ntransforms.py\n(\nhere\n), and\nmixtures.py\n(\nhere\n).Details in\nOpenvla-oft official documentation\n## 4. Finetune model ¶\nbash finetune_aloha.sh\nBy default, the training process will not save merged weights. So you need to run\nmerge_lora.sh\nto merge lora weights if you want to use the checkpoint. If some\n.py\nfiles miss in the merged checkpoint, just copy them from the original checkpoint.\n## 5. Eval on RoboTwin ¶\nexample usage\nbash eval.sh ${task_name} ${task_config} ${checkpoint_path} ${seed} ${gpu_id} ${unnorm_key}\n# Example: bash eval.sh move_can_pot demo_randomized ckpt_path 0 5 aloha_move_can_pot_builder\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root."}
{"id": "page-22", "contents": "LLaVA-VLA - RoboTwin 2.0 Offical Document\nSkip to content\n# LLaVA-VLA ¶\nContributed by IRPN Lab, HKUST(GZ)\nEmail: songwenxuan0115@gmail.com, sunxiaoquan@hust.edu.cn\n## 1. Environment Setup ¶\nSee\nLLaVA-VLA installation\nfor more details.\n## 2. Download Model ¶\nPlease download the corresponding model from the\nmodel zoo\n.\n## 3. Collect RoboTwin Data ¶\nSee\nRoboTwin Tutorial (Usage Section)\nfor more details.\n## 4. Generate Image and Data ¶\nFirst, create the pictures folder in the policy/LLaVA-VLA directory:\nmkdir pictures && training_data\ncd scripts && cd helper\nThen, extract the original image from RoboTwin data.\nbash image_extraction.sh ${task_name} ${task_config}\n# bash image_extraction.sh grab_roller demo_randomized\n# bash image_extraction.sh all demo_randomized\n# In task_name, you can directly select a task(such as: grab_roller) or choose \"all\" (just modify it in task_list).\nNext, generate the format data required for LLaVA-VLA training.\nbash process_data.sh ${task_name} ${task_config} ${future_chunk}\n# bash process_data.sh grab_roller demo_randomized 5\n# bash process_data.sh all demo_randomized 5\n# In task_name, you can directly select a task(such as: grab_roller) or choose \"all\" (just modify it in task_list).\n# future_chunk: The number of output steps in the future (default is 5).\nExample folder structure:\ntraining_data\n├── ${task_1}\n│ ├── ${task_config_1}\n| | |── episode0.json\n| | |── episode1.json\n│ ├── ${task_config_2}\n| | |── episode0.json\n| | |── episode1.json\n├── ${task_2}\n│ ├── ...\n├── ...\npictures\n├── ${task_1}\n│ ├── ${task_config_1}\n| | |── episode0\n| | | |── 01.jpg\n| | | |── 02.jpg\n│ ├── ${task_config_2}\n| | |── episode0\n| | | |── 01.jpg\n| | | |── ...\n├── ${task_2}\n│ ├── ...\n├── ...\n## 5. merge json and Generate yaml file ¶\nIn this step, we need to merge all the JSON files generated by the previous\nprocess_data\nstep into a single JSON file.\npython llava/process_data/merge_json.py\n# please replace `yourpath` with your actual path!\npython llava/process_data/yaml_general.py\n## 6. Pre-Training ¶\nBefore starting the training, please replace\nyourpath\nwith your actual path!\nbash calvin_finetune_obs.sh\n## 7. Fine-tuning ¶\nPlease note to change\nMODEL_NAME_OR_PATH\nto the checkpoint generated in the previous step. For the dataset you fine-tuned, please regenerate the\nACTION_STAT\nfile and modify\nJSON_PATH\n.Then\nbash calvin_finetune_obs.sh\n## 8. Eval on RoboTwin ¶\nYou need to modify the corresponding path in the deploy_policy.yml file: 1.\nmodel_path\n: Path to the checkpoint. 2.\naction_stat\n: Path to dataset_statistic.yaml.\nbash eval.sh ${gpu_id}\n# bash eval.sh 0\nThe evaluation results, including videos, will be saved in the\neval_result\ndirectory under the project root.\n## 9. Citation ¶\nIf you find our works useful for your research and applications, please cite using these BibTeX:\n@article{pdvla,\ntitle={Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding},\nauthor={Song, Wenxuan and Chen, Jiayi and Ding, Pengxiang and Zhao, Han and Zhao, Wei and Zhong, Zhide and Ge, Zongyuan and Ma, Jun and Li, Haoang},\njournal={arXiv preprint arXiv:2503.02310},\nyear={2025}\n}"}
{"id": "page-23", "contents": "Expert Code Gen (for Novel Task) - RoboTwin 2.0 Offical Document\nSkip to content\n# Expert Code Generation ¶\n## 1. Code_gen Folder Structure ¶\nThis directory contains various modules for generating and testing robot task code:\ngpt_agent.py\n: API integration with LLM models\nobservation_agent.py\n: Processes multi-modal observations for code correction\nprompt.py\n: Prompt templates for code generation\nrun_code.py\n: Executes and tests generated code\ntask_generation_simple.py\n: Basic single-pass code generation\ntask_generation.py\n: Iterative code generation with error feedback\ntask_generation_mm.py\n: Advanced code generation with multi-modal observation\ntask_info.py\n: Task definitions and descriptions\ntest_gen_code.py\n: Utility for testing generated code with detailed metrics\nThe code generation system also interacts with these important directories: -\n./envs/\n: Contains manually implemented task environments -\n_base_task.py\n: Core environment with robot control functions and utilities - Includes\nsave_camera_images(task_name, step_name, generate_num_id, save_dir)\nfor capturing visual observations during task execution -\n./envs_gen/\n: Stores auto-generated task implementations -\n./task_config/\n: Configuration files for tasks and embodiments -\n./script/\n: Template scripts and utilities -\n./assets/objects/\n: 3D models and metadata for simulation objects -\n./camera_images/\n: Stores observation images captured during code generation for multi-modal feedback\nThe entire pipeline enables automatic generation of robot control code from natural language task descriptions, with feedback-based refinement and multi-modal observation capabilities.\n## 2. Configure LLM API Key ¶\nPlease configure the necessary API keys in the\ncode_gen/gpt_agent.py\nfile. Additionally, if the LLM you are utilizing does not support integration with the OpenAI API, you may need to make corresponding adjustments to the\ngenerate()\nfunction.\n## 3. Generate Your Task Code ¶\n### 3.1 1. Add Task Description ¶\nAdd new task information, including the task name and natural language description, in\n./code_gen/task_info.py\n.\n# 1. Template of Task Information: ¶\nTASK_NAME\n=\n{\n\"task_name\"\n:\n\"task_name\"\n,\n# Name of the task\n\"task_description\"\n:\n\"...\"\n,\n# Detailed description of the task\n\"current_code\"\n:\n'''\nclass gpt_\n{task_name}\n(\n{task_name}\n):\ndef play_once(self):\npass\n'''\n# Code template to be completed\n\"actor_list\"\n:\n{\n# List of involved objects; can be a dictionary or a simple list\n\"self.object1\"\n:\n{\n\"name\"\n:\n\"object1\"\n,\n# Object name\n\"description\"\n:\n\"...\"\n,\n# Description of the object\n\"modelname\"\n:\n\"model_name\"\n# Name of the 3D model representing the object\n},\n\"self.object2\"\n:\n{\n\"name\"\n:\n\"object2\"\n,\n\"description\"\n:\n\"...\"\n,\n\"modelname\"\n:\n\"model_name\"\n},\n# ... more objects\n},\n# Alternatively, the actor_list can be a simple list:\n# \"actor_list\": [\"self.object1\", \"self.object2\", ...],\n# To make code generation easier, the actor_list also includes some pose information\n# like target poses or middle poses (optional and don't require modelname).\n}\n### 1.1 2. Add Basic Task Code ¶\nAdd the basic code file\n${task_name}.py\nin the\n./envs/\ndirectory, following this structure:\nfrom\n.base_task\nimport\nBase_task\nfrom\n.utils\nimport\n*\nimport\nsapien\nclass\n${\ntask_name\n}(\nBase_task\n):\ndef\nsetup_demo\n(\nself\n,\n**\nkwargs\n):\n# Initializes the simulation environment for the task\n# Sets up the table, robot, planner, camera, and initial positions\n# This function is called once at the beginning of each episode\npass\ndef\nload_actors\n(\nself\n):\n# Loads all the necessary objects for the task into the environment\n# Typically called from setup_demo to initialize scene objects\n# Can also be used to set initial poses for objects\npass\ndef\nplay_once\n(\nself\n):\n# Contains the robot control code to complete the task\n# This is the main function that will be generated by the LLM\n# Implements the sequence of actions for the robot to achieve the task\npass\n# Check success\ndef\ncheck_success\n(\nself\n):\n# Defines criteria to determine if the task was completed successfully\n# Returns a boolean indicating success or failure\n# Used for evaluation and feedback during code generation\npass\nIn the code above,\n{task_name}\nshould match the name of the basic code file, and the\ncheck_success()\nfunction is used to determine if the task is successful. No changes are needed for the rest of the code.\nNote: The\nenvs\nfolder contains manually written files with\nsetup_demo\n, robot operation code in\nplay_once\n, and\ncheck_success\nmethods. Auto-generated code will be saved in the\nenvs_gen\nfolder.\n### 1.2 3. Generate the Final Code ¶\nYou can use three different code generation approaches depending on your needs:\nNote: The code generation process will only generate the\nplay_once()\nmethod implementation, which contains the robot control logic to complete the task. Other methods like\nsetup_demo()\n,\nload_actors()\n, and\ncheck_success()\nshould be manually implemented.\n#### 1.2.1 Basic Code Generation ¶\nFor quick verification of new tasks or debugging existing ones without iterative correction:\npython\ncode_gen/task_generation_simple.py\ntask_name\n#### 1.2.2 Code Generation with Error Feedback ¶\nThis script implements iterative code correction based on error feedback, consistent with RoboTwin 1.0:\npython\ncode_gen/task_generation.py\ntask_name\n#### 1.2.3 Advanced Code Generation with Multi-Modal Observations ¶\nThis script provides both error feedback iteration and multi-modal observation-based code correction, consistent with RoboTwin 2.0. It offers the best generation quality but runs slower:\npython\ncode_gen/task_generation_mm.py\ntask_name\nThe multi-modal observation functionality is implemented in\ncode_gen/observation_agent.py\n.\nThe generated code file will be saved as\n./envs_gen/gpt_${task_name}.py\n. For example:\npython\ncode_gen/task_generation_mm.py\npick_dual_bottles_easy\nThis will create\n./envs_gen/gpt_pick_dual_bottles_easy.py\n.\n### 1.3 4. Test Generated Code ¶\nRun the following script to test the generated code:\npython\ncode_gen/run_code.py\ntask_name\nThis will execute the task using the generated code and display the results, allowing you to validate the performance.\n## 1.1 Additional Resources ¶\nFor more information on generating task descriptions and object descriptions, refer to the documentation in the\ndescription\ndirectory.\nFor policy training and evaluation using the generated code, consult the\npolicy/ACT\ndocumentation."}
{"id": "page-24", "contents": "API Tutorial - RoboTwin 2.0 Offical Document\nSkip to content\n# API for Controlling Mechanical Arms ¶\nThe API can be used to control one or two robotic arms to perform operations such as grasping, placing, moving, and returning to the origin. Each arm is identified by an\nArmTag\n, which can be\n\"left\"\nor\n\"right\"\n. Actions are generated in sequences and executed together via the\nmove()\nmethod.\n## 1. Class Structure ¶\nself\n: The task class inherit from\nBase_Task\n.\nArmTag\n: A custom type representing a robotic arm. It supports comparison with strings:\nArmTag(\"left\") == \"left\"\nreturns\nTrue\n. You can obtain the opposite arm using\nArmTag(\"left\").opposite\n, i.e.,\nArmTag(\"left\").opposite == \"right\"\nreturns\nTrue\n.\nActor\n/\nArticulationActor\n: The object being manipulated. Provides methods to retrieve key points (contact point\ncontact_point\n, functional point\nfunctional_point\n, target point\ntarget_point\n) and its current global pose.\nAction\n: A sequence of actions for controlling the arm. You only need to know that it can be executed via the\nmove()\nfunction.\n## 2. Controlling APIs ¶\n### 2.1 move(self, actions_by_arm1: tuple[ArmTag, list[Action]], actions_by_arm2: tuple[ArmTag, list[Action]] = None) ¶\n#### 2.1.1 Description ¶\nExecutes action sequences on one or both robotic arms simultaneously.\n#### 2.1.2 Parameters ¶\nactions_by_arm1\n: Action sequence for the first arm, formatted as\n(arm_tag, [action1, action2, ...])\nactions_by_arm2\n: Optional, action sequence for the second arm\n#### 2.1.3 Notes ¶\nThe same\nArmTag\ncannot be passed twice.\nAll actions must have been pre-generated.\n#### 2.1.4 Example ¶\nOne arm grasps a bottle, the other moves back to avoid interference.\nself\n.\nmove\n(\nself\n.\ngrasp_actor\n(\nself\n.\nbottle\n,\narm_tag\n=\narm_tag\n),\nself\n.\nback_to_origin\n(\narm_tag\n=\narm_tag\n.\nopposite\n)\n)\n### 2.2 grasp_actor(self, actor: Actor, arm_tag: ArmTag, pre_grasp_dis=0.1, grasp_dis=0, gripper_pos=0., contact_point_id=None) -> tuple[ArmTag, list[Action]] ¶\n#### 2.2.1 Description ¶\nGenerates a sequence of actions to pick up the specified\nActor\n.\n#### 2.2.2 Parameters ¶\nactor\n: The object to grasp\narm_tag\n: Which arm to use\npre_grasp_dis\n: Pre-grasp distance (default 0.1 meters), the arm will move to this position first\ngrasp_dis\n: Grasping distance (default 0 meters), the arm moves from the pre-grasp position to this position and then closes the gripper\ngripper_pos\n: Gripper closing position (default 0, fully closed)\ncontact_point_id\n: Optional list of contact point IDs; if not provided, the best grasping point is selected automatically\n#### 2.2.3 Returns ¶\n(arm_tag, action_list)\ncontaining the grasp actions.\n#### 2.2.4 Example ¶\nSelect appropriate grasp point based on arm_tag and grasp the cup.\nself\n.\nmove\n(\nself\n.\ngrasp_actor\n(\nself\n.\ncup\n,\narm_tag\n=\narm_tag\n,\npre_grasp_dis\n=\n0.1\n,\ncontact_point_id\n=\n[\n0\n,\n2\n][\nint\n(\narm_tag\n==\n'left'\n)]\n)\n)\n### 2.3 place_actor(self, actor: Actor, arm_tag: ArmTag, target_pose: list | np.ndarray, functional_point_id: int = None, pre_dis=0.1, dis=0.02, is_open=True, **kwargs) -> tuple[ArmTag, list[Action]] ¶\n#### 2.3.1 Description ¶\nPlaces a currently held object at a specified target pose.\n#### 2.3.2 Parameters ¶\nactor\n: The currently held object\narm_tag\n: The arm holding the object\ntarget_pose\n: Target position/orientation, length 3 or 7 (xyz + optional quaternion)\nfunctional_point_id\n: Optional ID of the functional point; if provided, aligns this point to the target, otherwise aligns the base of the object\npre_dis\n: Pre-place distance (default 0.1 meters), arm moves to this position first\ndis\n: Final placement distance (default 0.02 meters), arm moves from pre-place to this location, then opens the gripper\nis_open\n: Whether to open the gripper after placing (default True)\n**kwargs\n: Other optional parameters:\nconstrain : {'free', 'align', 'auto'}, default='auto'\nAlignment strategy:\nfree\n: Only forces the object's z-axis to align with the target point's z-axis, other axes are determined by projection.\nalign\n: Forces all axes of the object to align with all axes of the target point.\nauto\n: Automatically selects a suitable placement pose based on grasp direction (vertical or horizontal).\nalign_axis : list of np.ndarray or np.ndarray or list, optional\nVectors or vector list in world coordinates to align with. For example,\n[1, 0, 0]\nor\n[[1, 0, 0], [0, 1, 0]]\n. If multiple vectors are provided, the one with the smallest dot product with the current actor axis will be chosen for alignment.\nactor_axis : np.ndarray or list, default=[1, 0, 0]\nThe second object axis used for alignment (the first is the z-axis which will be forced to align). Typically used for auxiliary alignment (especially when\nconstrain == 'align'\n).\nactor_axis_type : {'actor', 'world'}, default='actor'\nSpecifies whether\nactor_axis\nis relative to the object coordinate system or world coordinate system.\npre_dis_axis : {'grasp', 'fp'} or np.ndarray or list, default='grasp'\nSpecifies the pre-placement offset direction:\ngrasp\n: Offset along the grasp direction (i.e., opposite to the end-effector pointing towards the object center).\nfp\n: Offset along the target point's z-axis direction.\nCustom vectors can also be provided to represent the offset direction.\n#### 2.3.3 Returns ¶\n(arm_tag, action_list)\ncontaining the place actions.\n#### 2.3.4 Example ¶\nWhen stacking one object on top of another (for example, placing blockA on top of blockB).\ntarget_pose\n=\nself\n.\nlast_actor\n.\nget_functional_point\n(\npoint_id\n,\n\"pose\"\n)\n# Use this target_pose in place_actor to place the object exactly on top of last_actor at the specified functional point.\nself\n.\nmove\n(\nself\n.\nplace_actor\n(\nactor\n=\nself\n.\ncurrent_actor\n,\n# The object to be placed\ntarget_pose\n=\ntarget_pose\n,\n# The pose acquired from last_actor\narm_tag\n=\narm_tag\n,\nfunctional_point_id\n=\n0\n,\n# Align functional point 0, or specify as needed\npre_dis\n=\n0.1\n,\ndis\n=\n0.02\n,\npre_dis_axis\n=\n\"fp\"\n,\n# Use functional point direction for pre-displacement, if the functional point is used\n)\n)\nPlace the actor at actor_pose (already a Pose object).\nself\n.\nmove\n(\nself\n.\nplace_actor\n(\nself\n.\nbox\n,\ntarget_pose\n=\nself\n.\nactor_pose\n,\n# already a Pose, no need for get_pose()\narm_tag\n=\ngrasp_arm_tag\n,\nfunctional_point_id\n=\n0\n,\n# functional_point_id can be retrived from the actor list if the actor has functional points\npre_dis\n=\n0\n,\ndis\n=\n0\n,\n# set dis to 0 if is_open is False, and the gripper will not open after placing. Set the `dis` to a small value like 0.02 if you want the gripper to open after placing.\nis_open\n=\nFalse\n,\n# if is_open is False, pre_dis and dis will be 0, and the gripper will not open after placing.\nconstrain\n=\n\"free\"\n,\n# if task requires the object to be placed in a specific pose that mentioned in the task description (like \"the head of the actor should be toward xxx), you can set constrain to \"align\", in all of other cases, you should set constrain to \"free\".\npre_dis_axis\n=\n'fp'\n,\n# Use functional point direction for pre-displacement, if the functional_point_id is used\n)\n)\n### 2.4 move_by_displacement(self, arm_tag: ArmTag, x=0., y=0., z=0., quat=None, move_axis='world') -> tuple[ArmTag, list[Action]] ¶\n#### 2.4.1 Description ¶\nMoves the end-effector of the specified arm along relative directions and sets its orientation.\n#### 2.4.2 Parameters ¶\narm_tag\n: The arm to control\nx\n,\ny\n,\nz\n: Displacement along each axis (in meters)\nquat\n: Optional quaternion specifying the target orientation; if not set, uses current orientation\nmove_axis\n:\n'world'\nmeans displacement is in world coordinates,\n'arm'\nmeans displacement is in local coordinates\n#### 2.4.3 Returns ¶\n(arm_tag, action_list)\ncontaining the move-by-displacement actions.\n#### 2.4.4 Example ¶\nLift the object up by moving relative to current position, you should lift the arm up evrery time after grasping an object to avoid collision.\nself\n.\nmove\n(\nself\n.\nmove_by_displacement\n(\narm_tag\n=\narm_tag\n,\nz\n=\n0.07\n,\n# Move 7cm upward\nmove_axis\n=\n'world'\n)\n)\n### 2.5 move_to_pose(self, arm_tag: ArmTag, target_pose: list) -> tuple[ArmTag, list[Action]] ¶\n#### 2.5.1 Description ¶\nMoves the end-effector of the specified arm to a specific absolute pose.\n#### 2.5.2 Parameters ¶\narm_tag\n: The arm to control\ntarget_pose\n: Absolute position and/or orientation, length 3 or 7 (xyz + optional quaternion)\n#### 2.5.3 Returns ¶\n(arm_tag, action_list)\ncontaining the move-to-pose actions.\n#### 2.5.4 Example ¶\nMove the arm to a specific pose, for example, to place an object in a certain position decided by which arm is placing the object.\ntarget_pose\n=\nself\n.\nget_arm_pose\n(\narm_tag\n=\narm_tag\n)\nif\narm_tag\n==\n'left'\n:\n# Set specific position and orientation for left arm\ntarget_pose\n[:\n2\n]\n=\n[\n-\n0.1\n,\n-\n0.05\n]\ntarget_pose\n[\n2\n]\n-=\n0.05\ntarget_pose\n[\n3\n:]\n=\n[\n-\n0.707\n,\n0\n,\n-\n0.707\n,\n0\n]\nelse\n:\n# Set specific position and orientation for right arm\ntarget_pose\n[:\n2\n]\n=\n[\n0.1\n,\n-\n0.05\n]\ntarget_pose\n[\n2\n]\n-=\n0.05\ntarget_pose\n[\n3\n:]\n=\n[\n0\n,\n0.707\n,\n0\n,\n-\n0.707\n]\n# Move the skillet to the defined target pose\nself\n.\nmove\n(\nself\n.\nmove_to_pose\n(\narm_tag\n=\narm_tag\n,\ntarget_pose\n=\ntarget_pose\n)\n)\n### 2.6 close_gripper(self, arm_tag: ArmTag, pos=0.) -> tuple[ArmTag, list[Action]] ¶\n#### 2.6.1 Description ¶\nCloses the gripper of the specified arm.\n#### 2.6.2 Parameters ¶\narm_tag\n: Which arm's gripper to close\npos\n: Gripper position (0 = fully closed)\n#### 2.6.3 Returns ¶\n(arm_tag, action_list)\ncontaining the gripper-close action.\n#### 2.6.4 Example ¶\nself\n.\nmove\n(\nself\n.\nclose_gripper\n(\narm_tag\n=\narm_tag\n)\n)\n### 2.7 open_gripper(self, arm_tag: ArmTag, pos=1.) -> tuple[ArmTag, list[Action]] ¶\n#### 2.7.1 Description ¶\nOpens the gripper of the specified arm.\n#### 2.7.2 Parameters ¶\narm_tag\n: Which arm's gripper to open\npos\n: Gripper position (1 = fully open)\n#### 2.7.3 Returns ¶\n(arm_tag, action_list)\ncontaining the gripper-open action.\n#### 2.7.4 Example ¶\nself\n.\nmove\n(\nself\n.\nopen_gripper\n(\narm_tag\n=\narm_tag\n)\n)\n### 2.8 back_to_origin(self, arm_tag: ArmTag) -> tuple[ArmTag, list[Action]] ¶\n#### 2.8.1 Description ¶\nReturns the specified arm to its predefined initial position.\n#### 2.8.2 Parameters ¶\narm_tag\n: The arm to return to origin\n#### 2.8.3 Returns ¶\n(arm_tag, action_list)\ncontaining the return-to-origin action.\n#### 2.8.4 Example ¶\nPlace left object while moving right arm back to origin.\nmove_arm_tag\n=\nArmTag\n(\n\"left\"\n)\n# Specify which arm is placing the object\nback_arm_tag\n=\nArmTag\n(\n\"right\"\n)\n# Specify which arm is moving back to origin\nself\n.\nmove\n(\nself\n.\nplace_actor\n(\nactor\n=\nself\n.\nleft_actor\n,\narm_tag\n=\nmove_arm_tag\n,\ntarget_pose\n=\ntarget_pose\n,\npre_dis_axis\n=\n\"fp\"\n,\n),\nself\n.\nback_to_origin\n(\narm_tag\n=\nback_arm_tag\n)\n)\n### 2.9 get_arm_pose(self, arm_tag: ArmTag) -> list[float] ¶\n#### 2.9.1 Description ¶\nGets the current pose of the end-effector of the specified arm.\n#### 2.9.2 Parameters ¶\narm_tag\n: Which arm to query\n#### 2.9.3 Returns ¶\nA list of 7 floats:\n[x, y, z, qw, qx, qy, qz]\n, representing position and orientation.\n#### 2.9.4 Example ¶\npose\n=\nself\n.\nget_arm_pose\n(\nArmTag\n(\n\"left\"\n))\n## 3. Actor Class APIs ¶\nActor\nis the object being manipulated by the robotic arms. It provides methods to retrieve key points and its current global pose. The\nActor\nclass has the following data points:\nTarget Point\ntarget_point\n: Special points available during planning (e.g., handle of a cup)\nContact Point\ncontact_point\n: Position where the robotic arm grasps the object (e.g., rim of a cup)\nFunctional Point\nfunctional_point\n: Position where the object interacts with other objects (e.g., head of a hammer)\nOrientation Point\norientation_point\n: Specifies the orientation of the object (e.g., toe of a shoe pointing left)\nThese methods can be called on\nActor\nobjects:\n### 3.1 get_contact_point(self, idx: int) -> list[float] ¶\nReturns the pose of the\nidx\n-th contact point as\n[x, y, z, qw, qx, qy, qz]\n### 3.2 get_functional_point(self, idx: int) -> list[float] ¶\nReturns the pose of the\nidx\n-th functional point as\n[x, y, z, qw, qx, qy, qz]\n### 3.3 get_target_point(self, idx: int) -> list[float] ¶\nReturns the pose of the\nidx\n-th target point as\n[x, y, z, qw, qx, qy, qz]\n### 3.4 get_orientation_point(self, idx: int) -> list[float] ¶\nReturns the pose of the\nidx\n-th orientation point as\n[x, y, z, qw, qx, qy, qz]\n### 3.5 get_pose(self) -> sapien.Pose ¶\nReturns the global pose of the object in SAPIEN (\n.p\nis position,\n.q\nis orientation)\n## 4. ArticulationActor Class APIs ¶\nIf the actor was created with method that contains \"urdf\"(e.g.\ncreate_rand_sapien_urdf_actor\n), it will be a subclass of\nActor\ncalled\nArticulationActor\n, with the following additional methods:\n### 4.1 get_qlimits(self) -> list[tuple[float, float]] ¶\nReturns a list of joint limits, where each joint limit is a tuple\n(min, max)\n.\n### 4.2 get_qpos(self) -> list[float] ¶\nReturns the current positions (rotational/positional) of all joints.\n### 4.3 get_qvel(self) -> list[float] ¶\nReturns the current velocities of all joints."}
{"id": "page-25", "contents": "Dual Env (Policy and Deployment) - RoboTwin 2.0 Offical Document\nSkip to content\n# Dual Env Policy Deploy ¶"}
{"id": "page-26", "contents": "Description Gen (Object & Task) - RoboTwin 2.0 Offical Document\nSkip to content\n# Description Gen (Object & Task) ¶\n## 1. Object Description ¶\n# Generate object description for all objects\npython3\nutils/generate_object_description.py\n# Generate object description for a specific type of object with as many objects as this class contains\npython3\nutils/generate_object_description.py\n001_bottle\n# Generate object description for a specific object index of a specific type of object\npython3\nutils/generate_object_description.py\n001_bottle\n--index\n0\n## 2. Task Instruction ¶\n# Generate 60 task descriptions for a task\npython3\nutils/generate_task_description.py\nplace_shoe\n60\nIt will call for\ninstruction_num % 12\ntimes of API, each time returning 12 instructions shuffled into 10 seen and 2 unseen instructions.\n## 3. Episode Instruction ¶\n# Generate 60 task descriptions for a task\npython3\nutils/generate_episode_instructions.py\nplace_shoe\nfranka-panda-D435\n1000\n### 3.1 Parameters: ¶\ntask_name\n: Name of the task (JSON file name without extension)\nsetting\n: Setting name used to construct the data directory path\nmax_num\n: Maximum number of descriptions per episode"}
{"id": "page-27", "contents": "Object Annotation - RoboTwin 2.0 Offical Document\nSkip to content\n# Calibration Tool Instructions ¶\n## 1. Rigid Body Object Annotation ¶\n### 1.1 Create Calibration Window: ¶\npython\nscript/create_object_data.py\n[\n-s\nSTART\n]\nmodel_name\npositional\narguments:\nmodel_name\nModel\nname\noptions:\n-s\nSTART,\n--start\nSTART\nStart\nid\nHere,\nmodel_name\nis the name of a subdirectory under the\nassets/objects/\ndirectory. For example, to calibrate the hammer model located at\nassets/objects/020_hammer\n, run the command:\npython script/create_object_data.py 020_hammer\n. A window will then appear as shown below:\n### 1.2 Calibration Commands: ¶\nresize:\nUsage:\nresize <x_size> <y_size> <z_size>: Set scaling along x, y, z axes\nresize <size>: Uniformly scale all three axes\nExample:\nresize 0.1\ncreate:\nUsage:\ncreate <type>: Create (t)arget, (c)ontact, (f)unctional, or (o)rientation point\ncreate: Waits for input of point name\nExamples:\ncreate t\ncreate f\nclone:\nUsage:\nclone <type> <id>: Clone a specified type and ID point in place\nclone: Waits for input of point type and ID\nExamples:\nclone t 1: Clones contact point target_1 to create a new target point (e.g., target_2)\nrotate:\nUsage:\nrotate <id> <axis> <interval>: Rotate a specified contact point around its own axis by a given interval, generating points belonging to the same group\nExample:\nrotate 1 x 90: Rotates contact_1 around its x-axis every 90 degrees, creating three additional contact points, and writes the group into concat_points_group\nalign:\nUsage:\nalign: Aligns all group points' positions to the first point in the group\nremove:\nUsage:\nremove <type> <id>: Removes a point with the specified name\nremove: Waits for input of point name\nExamples:\nremove t 0\nsave:\nSaves current calibration data — always remember to save!\nexit:\nExits the calibration window\nAs an example using\n020_hammer\n, entering\ncreate c\ncreates a cube centered on the object. Use your mouse to select this cube and check \"Enable\" under the Transform section in the UI window. Then choose \"Local\" to display the cube's center position and coordinate system, which represents the contact point's location and orientation:\nYou can move the calibration point's position with the mouse. Click on \"Rotate\" in the Transform options to adjust the rotation along the x, y, and z axes, changing the point's coordinate system orientation:\nNext, add a functional point to the head of the hammer, adjust its orientation, and use the command\ncreate f\nto move it to the center position of the hammer head. The adjusted point is shown in the following image:\nFinally, enter\nsave\nto save the point information, and then enter\nexit\nto end the calibration.\nNotes :\nAfter adjusting the position, you must click \"Teleport\" under the Transform menu to apply the movement.\nAlways remember to save your changes before exiting the calibration window!\n### 1.3 View Calibration Files ¶\nNavigate to the asset folder you just calibrated, and you will find a newly generated\nmodel_data{id}.json\nfile. You can modify the\n\"scale\"\nfield within this file to adjust the asset's scaling in the simulation environment.\nThe meanings of each field in the asset can be found in the\nmodel_data_info\nfile.\n## 2. URDF Articulation Objects Annotation ¶\n### 2.1 Create Calibration Window: ¶\nSimilar to rigid body object annotation, use the same command to create the articulation calibration window. The calibration program will automatically recognize the asset type.\n### 2.2 Calibration Commands: ¶\nrun:\nUsage:\nrun\nPress <Ctrl + C> to stop and save information\nUsed to obtain stable points through steps, generally selected at the beginning of calibration to determine if running is necessary.\nSince this command does not limit the step upper limit, you need to manually stop running (press Ctrl+C) based on whether the asset in the UI interface is stable.\nqpos:\nUsage:\nqpos\nGet the current joint state as the initial pose when loading the asset into the task.\nmass:\nUsage:\nmass <m1> <m2> ...: Set the mass of the articulation joint, ensuring that the input matches the displayed link count (excluding base) in order.\nExample:\nmass 0.5 0.05\nresize:\nUsage:\nresize <size>: Synchronize the scaling of all three axes of the object\nExample:\nresize 0.1\ncreate:\nUsage:\ncreate <type> <base_link>: Create (t)arget, (c)ontact, (f)unctional, (o)rientation points\nExample:\ncreate c link1\nrebase:\nUsage:\nrebase <type> <id> <base_link>: Modify the base link of the specified point\nExample:\nrebase c 0 link1\nclone:\nUsage:\nclone <type> <id>: Create an in-place copy of the specified point (without base)\nExample:\nclone t 1: Create a new target point (e.g., target_2<link1>) by copying target_1<link1>\nrotate:\nUsage:\nrotate <id> <axis> <interval>: Rotate the specified contact point around its own specified axis by the specified interval, generating points belonging to the same group\nExample:\nrotate 1 x 90: Rotate contact_1 around its own x-axis by 90 degrees, generating three contact points, and write the grouping of the four points into concat_points_group\nalign:\nUsage:\nalign: Align the positions of all group points to the first point in the group\nremove:\nUsage:\nremove <type> <id>: Remove the specified point (without base)\nExample:\nremove t 0\nsave:\nUsage:\nsave: Save the current calibration data, and make sure to save!\nexit:\nUsage:\nexit: Exit the calibration window\nThe calibration process is similar to rigid body object annotation, and you also need to save the data and exit after completion."}
{"id": "page-28", "contents": "Configuring New Embodiment - RoboTwin 2.0 Offical Document\nSkip to content\n# Configure New Embodiment in RoboTwin ¶\nEmbodiments are stored in the\nassets/embodiments\ndirectory. Each embodiment follows this file structure:\n# Using Franka as an example\n- embodiments\n- franka-panda\n- config.yml # RoboTwin config file\n- curobo_tmp.yml # CuRobo config template\n- collision_franka.yml # CuRobo collision annotations\n- urdf_files/... # URDF files and corresponding GLB, STL files, etc.\nThis guide explains how to configure a new embodiment from scratch, using Franka as an example.\n## 1. Step 1: Configure CuRobo Files ¶\nFor complete configuration instructions, refer to the official documentation: https://curobo.org/tutorials/1_robot_configuration.html. This section provides the minimal configuration steps.\n### 1.1 1.1 Create the embodiment directory and files ¶\ncd\n${\nROBOTWIN_ROOT_PATH\n}\nmkdir\n-p\nassets/embodiments/new_robot\ncd\nassets/embodiments/new_robot\ntouch\ncurobo_tmp.yml\ntouch\ncollision.yml\n### 1.2 1.2 Configure curobo_tmp.yml ¶\nHere's a minimal Franka configuration example:\nrobot_cfg:\nkinematics:\nurdf_path: ${ASSETS_PATH}/assets/embodiments/franka-panda/panda.urdf\nbase_link: \"panda_link0\"\nee_link: \"panda_hand\"\ncollision_link_names:\n[\n\"panda_link0\",\n\"panda_link1\",\n\"panda_link2\",\n\"panda_link3\",\n\"panda_link4\",\n\"panda_link5\",\n\"panda_link6\",\n\"panda_link7\",\n\"panda_hand\",\n\"panda_leftfinger\",\n\"panda_rightfinger\",\n\"attached_object\",\n]\ncollision_spheres: ${ASSETS_PATH}/assets/embodiments/franka-panda/collision_franka.yml\ncollision_sphere_buffer: 0.004\nself_collision_ignore: {...}\nself_collision_buffer: {...}\nmesh_link_names: [...]\nlock_joints: {\"panda_finger_joint1\": 0.04, \"panda_finger_joint2\": 0.04}\ncspace:\njoint_names: [\"panda_joint1\",\"panda_joint2\",\"panda_joint3\",\"panda_joint4\", \"panda_joint5\", \"panda_joint6\",\"panda_joint7\",\"panda_finger_joint1\", \"panda_finger_joint2\"]\nretract_config: [0.2200, -1.4012, -0.0406, -1.4901, 0.3050, 0.4521, 0.2099, 0.04, 0.04]\nnull_space_weight: [1,1,1,1,1,1,1,1,1]\ncspace_distance_weight: [1,1,1,1,1,1,1,1,1]\nmax_acceleration: 15.0\nmax_jerk: 500.0\nplanner:\nframe_bias: [0., 0., 0.]\nKey Parameter Explanations:\nPath Requirements\n: Since this is a config template and CuRobo only supports absolute paths, both\nurdf_path\nand\ncollision_spheres\nmust keep the\n${ASSETS_PATH}/assets/embodiments/\nprefix unchanged. The\n${ASSETS_PATH}\nvariable will be automatically replaced with the absolute path during subsequent operations.\nbase_link and ee_link\n: These are the two most important links that directly determine your planning space. Replace these with your robot arm's actual link names.\nCollision Configuration\n:\ncollision_link_names\nand\ncollision_spheres\ndetermine self-collision and environment collision detection during planning. For detailed configuration, refer to the \"Robot Collision Representation\" section at https://curobo.org/tutorials/1_robot_configuration.html. All configurations in this repository are based on Isaac Sim 4.2.\nJoint Configuration\n:\ncspace/joint_names\ndirectly determines which joints need planning. This is defined by the URDF and must match the corresponding joint names. The lengths of\nretract_config\n,\nnull_space_weight\n, and\ncspace_distance_weight\nmust match the length of\njoint_names\n.\nFrame Bias\n: For single-arm URDFs, keep\nplanner/frame_bias\nas\n[0., 0., 0.]\n. For dual-arm setups like ALOHA, slight adjustments are needed (detailed in the\ndual-arm configuration section\n).\n### 1.3 1.3 Configure collision.yml ¶\nAfter annotating with Isaac Sim, you'll get collision spheres for different joints. Fill them into collision.yml in this format:\ncollision_spheres:\npanda_link0:\n- \"center\": [0.0, 0.0, 0.085]\n\"radius\": 0.03\n# ... more spheres\npanda_link1:\n- \"center\": [0.0, -0.08, 0.0]\n\"radius\": 0.035\n# ... more spheres\n### 1.4 1.4 Verify CuRobo Configuration ¶\nAfter configuring CuRobo, verify the setup with a simple forward kinematics test. First, update the\n${ASSETS_PATH}\n:\ncd\n${\nROBOTWIN_ROOT_PATH\n}\npython\nscript/update_embodiment_config_path.py\nThis will generate\ncurobo.yml\nfrom\ncurobo_tmp.yml\n. Then run this verification code:\nimport\ntorch\nfrom\ncurobo.cuda_robot_model.cuda_robot_model\nimport\nCudaRobotModel\n,\nCudaRobotModelConfig\nfrom\ncurobo.types.base\nimport\nTensorDeviceType\nfrom\ncurobo.types.robot\nimport\nRobotConfig\nfrom\ncurobo.util_file\nimport\nget_robot_path\n,\njoin_path\n,\nload_yaml\ntensor_args\n=\nTensorDeviceType\n()\n# Modify to the absolute path of `curobo.yml`\nconfig_file\n=\nload_yaml\n(\n\"/abs_path/to/curobo.yml\"\n)\nurdf_file\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"urdf_path\"\n]\nbase_link\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"base_link\"\n]\nee_link\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"ee_link\"\n]\nrobot_cfg\n=\nRobotConfig\n.\nfrom_basic\n(\nurdf_file\n,\nbase_link\n,\nee_link\n,\ntensor_args\n)\nkin_model\n=\nCudaRobotModel\n(\nrobot_cfg\n.\nkinematics\n)\nq\n=\ntorch\n.\nrand\n((\n10\n,\nkin_model\n.\nget_dof\n()),\n**\n(\ntensor_args\n.\nas_torch_dict\n()))\nout\n=\nkin_model\n.\nget_state\n(\nq\n)\nIf no errors occur, the configuration is successful.\n## 2. Step 2: Configure RoboTwin Config File ¶\n### 2.1 2.1 Create config.yml ¶\ncd\nassets/embodiments/new_robot\ntouch\nconfig.yml\n### 2.2 2.2 Parameter Configuration ¶\nHere's a Franka configuration example with detailed explanations:\nurdf_path: \"./panda.urdf\"\nsrdf_path: \"./panda.srdf\"\njoint_stiffness: 1000\njoint_damping: 200\ngripper_stiffness: 1000\ngripper_damping: 200\nmove_group: [\"panda_hand\",\"panda_hand\"]\nee_joints: [\"panda_hand_joint\",\"panda_hand_joint\"]\narm_joints_name: [['panda_joint1', 'panda_joint2', 'panda_joint3', 'panda_joint4', 'panda_joint5', 'panda_joint6', 'panda_joint7'],\n['panda_joint1', 'panda_joint2', 'panda_joint3', 'panda_joint4', 'panda_joint5', 'panda_joint6', 'panda_joint7']]\ngripper_name:\n- base: \"panda_finger_joint1\"\nmimic: [[\"panda_finger_joint2\", 1., 0.]]\n- base: \"panda_finger_joint1\"\nmimic: [[\"panda_finger_joint2\", 1., 0.]]\ngripper_bias: 0.08\ngripper_scale: [0.0, 0.04]\nhomestate: [[0, 0.19634954084936207, 0.0, -2.617993877991494, 0.0, 2.941592653589793, 0.7853981633974483],\n[0, 0.19634954084936207, 0.0, -2.617993877991494, 0.0, 2.941592653589793, 0.7853981633974483]]\ndelta_matrix: [[0,0,1],[0,-1,0],[1,0,0]]\nglobal_trans_matrix: [[1,0,0],[0,-1,0],[0,0,-1]]\nrobot_pose: [[0, -0.65, 0.75, 0.707, 0, 0, 0.707],\n[0, -0.65, 0.75, 0.707, 0, 0, 0.707]]\nplanner: \"curobo\"\ndual_arm: False\nrotate_lim: [0.1, 0.8]\ngrasp_perfect_direction: ['right', 'left']\nstatic_camera_list:\n- name: head_camera\nposition: [0.0, 0.8, 0.9]\nforward: [0, -1, 0]\nleft: [1, 0, 0]\nParameter Explanations for New Embodiments:\nurdf_path and srdf_path\n: Relative paths to URDF and SRDF files within\nassets/embodiments/new_robot\n. These are loaded by Sapien into the simulator and directly determine the physical collision properties.\nmove_group\n: Used by MPLib, equivalent to CuRobo's\nee_link\n. This is a list containing the ee_links for left and right arms.\nee_joints\n: Since Sapien only supports global pose reading for joints, use the parent joint of the link specified in\nmove_group\n.\narm_joints_name\n: Joint names, same as CuRobo's\njoint_names\nparameter, but organized as a 2D list containing joint names for both left and right arms.\ngripper_name\n: Controls gripper movement with structure:\nlist[dict{\"base\":str, \"mimic\":[[str, float, float], ...]}, dict{\"base\":str, \"mimic\":[[str, float, float], ...]}]\nFirst level list represents left and right grippers\nSecond level dict distinguishes \"base\" (actively controlled joint) and \"mimic\" (passive joints)\n\"base\": String representing any gripper finger, controlled by\ngripper_scale\nwhere\ngripper_scale[0]\nis closed state and\ngripper_scale[1]\nis open state\n\"mimic\": 2D array where each element contains [str, float1, float2] - joint name, scale, and bias. Joint angle = float1 * base_joint + bias\ngripper_bias\n: Adjusts distance from\nee_joint\nto gripper center. For example, in vertical downward grasping, larger values move the gripper down, smaller values move it up.\nhomestate\n: Initial robot arm state. Set carefully to avoid self-collision that could cause planning failures.\ndelta_matrix\n: Rotation matrix to unify different ee_joint coordinate systems. To avoid errors, initially use an identity matrix as placeholder:\n[[1,0,0],[0,1,0],[0,0,1]]\n.\nglobal_trans_matrix\n: Rotation matrix to unify ee_joint pose reading in Sapien. To avoid errors, initially use an identity matrix as placeholder:\n[[1,0,0],[0,1,0],[0,0,1]]\n.\nrobot_pose\n: Base_link placement positions in format\n[[x,y,z,qw,qx,qy,qz],[x,y,z,qw,qx,qy,qz]]\n. The x-coordinate represents the center position between two arms, recommended as 0. Actual spacing is adjusted in task configs like\ndemo_randomized.yml\n.\ndual_arm\n: Boolean indicating whether the URDF is dual-arm (true for ALOHA) or single-arm (false for Franka).\nstatic_camera_list\n: Adjusts head_camera position, where\nforward\nand\nleft\nrepresent the z-axis and x-axis directions of the camera coordinate system.\n## 3. Step 3: Add Embodiment Path ¶\nEdit\ntask_config/_embodiment_config.yml\nand add your new robot path:\nnew_robot:\nfile_path: \"./assets/embodiments/new_robot\"\nNote\n: Your\nconfig.yml\nand\ncurobo_tmp.yml\nmust be directly located under\nfile_path\n.\n## 4. Step 4: Modify Task Config ¶\nIn your task config (e.g.,\ntask_config/demo_randomized.yml\n), change the\nembodiment\nsection to:\nembodiment:\n- new_robot\n- new_robot\n- 0.8 # Distance between the two robot arms\n## 5. Step 5: Calibrate delta_matrix ¶\nThis calibration requires the desktop environment and is\nextremely important\n.\n### 5.1 5.1 Create Temporary URDF ¶\nBefore calibrating\ndelta_matrix\nand\nglobal_trans_matrix\n, you must create a temporary URDF. Using Franka as an example:\ncd\nassets/embodiments/franka-panda\ncp\npanda.urdf\npanda.urdf.save\nModify\npanda.urdf\nby: 1.\nRemove or comment out all collision tags\nfor every link 2.\nRemove all joint limits\nand change all\nrevolute\njoints to\ncontinuous\nExample modifications:\n<!-- Comment out collision -->\n<link\nname=\n\"panda_link1\"\n>\n<visual>\n<geometry>\n<mesh\nfilename=\n\"franka_description/meshes/visual/link1.glb\"\n/>\n</geometry>\n</visual>\n<!-- <collision>\n<geometry>\n<mesh filename=\"franka_description/meshes/collision/link1.stl\"/>\n</geometry>\n</collision> -->\n</link>\n<!-- Remove joint limits, change revolute to continuous -->\n<!-- <joint name=\"panda_joint3\" type=\"revolute\"> -->\n<joint\nname=\n\"panda_joint3\"\ntype=\n\"continuous\"\n>\n<origin\nrpy=\n\"1.57079632679 0 0\"\nxyz=\n\"0 -0.316 0\"\n/>\n<parent\nlink=\n\"panda_link2\"\n/>\n<child\nlink=\n\"panda_link3\"\n/>\n<axis\nxyz=\n\"0 0 1\"\n/>\n<!-- Remove this line: <limit effort=\"87\" lower=\"-2.8973\" upper=\"2.8973\" velocity=\"2.1750\"/> -->\n</joint>\n### 5.2 5.2 Find Valid Pose ¶\nThe\ndelta_matrix\nunifies coordinate systems across different robot arms. First, run this script to find a valid pose:\nimport\ntorch\nfrom\ncurobo.types.base\nimport\nTensorDeviceType\nfrom\ncurobo.types.math\nimport\nPose\nfrom\ncurobo.types.robot\nimport\nRobotConfig\nfrom\ncurobo.util_file\nimport\nget_robot_configs_path\n,\njoin_path\n,\nload_yaml\nfrom\ncurobo.wrap.reacher.ik_solver\nimport\nIKSolver\n,\nIKSolverConfig\ntensor_args\n=\nTensorDeviceType\n()\nconfig_file\n=\nload_yaml\n(\njoin_path\n(\nget_robot_configs_path\n(),\n\"franka.yml\"\n))\nurdf_file\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"urdf_path\"\n]\nbase_link\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"base_link\"\n]\nee_link\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"ee_link\"\n]\nrobot_cfg\n=\nRobotConfig\n.\nfrom_basic\n(\nurdf_file\n,\nbase_link\n,\nee_link\n,\ntensor_args\n)\nik_config\n=\nIKSolverConfig\n.\nload_from_robot_config\n(\nrobot_cfg\n,\nNone\n,\nnum_seeds\n=\n20\n,\nself_collision_check\n=\nFalse\n,\nself_collision_opt\n=\nFalse\n,\ntensor_args\n=\ntensor_args\n,\nuse_cuda_graph\n=\nTrue\n,\n)\nik_solver\n=\nIKSolver\n(\nik_config\n)\nx_values\n=\ntorch\n.\nlinspace\n(\n0.35\n,\n0.0\n,\n25\n)\n.\ntolist\n()\n+\ntorch\n.\nlinspace\n(\n0.35\n,\n0.7\n,\n25\n)\n.\ntolist\n()\ny_values\n=\ntorch\n.\nlinspace\n(\n0.25\n,\n0.0\n,\n25\n)\n.\ntolist\n()\n+\ntorch\n.\nlinspace\n(\n0.25\n,\n0.5\n,\n25\n)\n.\ntolist\n()\nz_values\n=\ntorch\n.\nlinspace\n(\n0.25\n,\n0.0\n,\n25\n)\n.\ntolist\n()\n+\ntorch\n.\nlinspace\n(\n0.25\n,\n0.5\n,\n25\n)\n.\ntolist\n()\nquaternion\n=\ntorch\n.\ntensor\n([[\n1.0\n,\n0.0\n,\n0.0\n,\n0.0\n]],\ndevice\n=\n'cuda:0'\n)\nprint\n(\n\"Testing IK solutions for different positions:\"\n)\nprint\n(\n\"x, y, z, success\"\n)\nfor\nx\nin\nx_values\n:\nfor\ny\nin\ny_values\n:\nfor\nz\nin\nz_values\n:\ngoal\n=\nPose\n(\nposition\n=\ntorch\n.\ntensor\n([[\nfloat\n(\nx\n),\nfloat\n(\ny\n),\nfloat\n(\nz\n)]],\ndevice\n=\n'cuda:0'\n),\nquaternion\n=\nquaternion\n)\nresult\n=\nik_solver\n.\nsolve_single\n(\ngoal\n)\nif\nresult\n.\nsuccess\n.\nitem\n()\n==\nTrue\n:\nprint\n(\nf\n\"\n{\nx\n:\n.2f\n}\n,\n{\ny\n:\n.2f\n}\n,\n{\nz\n:\n.2f\n}\n,\n{\nresult\n.\nsuccess\n}\n\"\n)\nExpected output:\nx, y, z, success\n0.35, 0.23, 0.09, tensor([[True]], device='cuda:0')\n0.35, 0.23, 0.08, tensor([[True]], device='cuda:0')\n0.35, 0.23, 0.07, tensor([[True]], device='cuda:0')\n...\n### 5.3 5.3 Test in Simulation ¶\nChoose any successful xyz coordinates and modify\nenvs/robot/planner.py\naround line 266:\ntarget_pose_p\n[\n0\n]\n+=\nself\n.\nframe_bias\n[\n0\n]\ntarget_pose_p\n[\n1\n]\n+=\nself\n.\nframe_bias\n[\n1\n]\ntarget_pose_p\n[\n2\n]\n+=\nself\n.\nframe_bias\n[\n2\n]\n## Temporarily add the successful xyz coordinates ##\ntarget_pose_p\n=\n[\n0.35\n,\n0.23\n,\n0.09\n]\n# Example: using 0.35, 0.23, 0.09\ntarget_pose_q\n=\n[\n1.\n,\n0.\n,\n0.\n,\n0.\n]\n## End temporary addition ##\ngoal_pose_of_gripper\n=\nCuroboPose\n.\nfrom_list\n(\nlist\n(\ntarget_pose_p\n)\n+\nlist\n(\ntarget_pose_q\n))\nModify\nenvs/beat_block_hammer.py\nto add a temporary test:\n######## Add temporary test ##########\narm_tag\n=\nArmTag\n(\n'left'\n)\naction\n=\nAction\n(\narm_tag\n,\n'move'\n,\n[\n-\n0.05\n,\n0.\n,\n0.9\n])\nself\n.\nmove\n((\narm_tag\n,\n[\naction\n]))\ntime\n.\nsleep\n(\n100\n)\n######################################\n# Grasp the hammer with the selected arm\nself\n.\nmove\n(\nself\n.\ngrasp_actor\n(\nself\n.\nhammer\n,\narm_tag\n=\narm_tag\n,\npre_grasp_dis\n=\n0.12\n,\ngrasp_dis\n=\n0.01\n,\ngripper_pos\n=\n0.35\n))\nSet\nrender_freq\nto a positive number in your task config (e.g.,\ndemo_randomized.yml\n), then run:\nbash\ncollect_data.sh\nbeat_block_hammer\ndemo_randomized\n0\n### 5.4 5.4 Analyze Coordinate Systems ¶\nYou should see a visualization similar to this:\nCoordinate System Analysis:\n-\nee_joint_frame\n: -\nX-axis (red)\n: Should point from the link toward the gripper direction -\nY-axis (green)\n: Should be parallel to gripper movement direction (positive or negative) -\nZ-axis (blue)\n: Determined by right-hand rule\nreference_frame\n:\nX-axis\n: Robot's forward direction\nZ-axis\n: Opposite to gravity direction (upward)\nY-axis\n: Determined by right-hand rule\nThis frame is fixed and consistent across all robots\n### 5.5 5.5 Calculate delta_matrix ¶\nThe\ndelta_matrix\nrepresents the rotation from ee_joint frame to reference frame:\n{ee_joint}_Rotation_{reference}\n.\nFrom the example image above, the delta_matrix would be:\ndelta_matrix = [[0, 0, 1],\n[0, -1, 0],\n[1, 0, 0]]\nUpdate this matrix in your\nconfig.yml\n.\n## 6. Step 6: Calibrate global_trans_matrix ¶\n### 6.1 6.1 Get Actual Planned Pose ¶\nKeep the\ntime.sleep\nin\nbeat_block_hammer.py\nand modify\nenvs/robot/planner.py\nto output the target quaternion:\ntarget_pose_p\n[\n0\n]\n+=\nself\n.\nframe_bias\n[\n0\n]\ntarget_pose_p\n[\n1\n]\n+=\nself\n.\nframe_bias\n[\n1\n]\ntarget_pose_p\n[\n2\n]\n+=\nself\n.\nframe_bias\n[\n2\n]\n# Remove the hardcoded position and quaternion\n# target_pose_p = np.array([0.35, 0.23, 0.09])\n# target_pose_q = np.array([1.0, 0.0, 0.0, 0.0])\nprint\n(\n'[debug]: target_pose_q: '\n,\ntarget_pose_q\n)\ngoal_pose_of_gripper\n=\nCuroboPose\n.\nfrom_list\n(\nlist\n(\ntarget_pose_p\n)\n+\nlist\n(\ntarget_pose_q\n))\nExpected output:\n[debug]: target_pose_q: [ 1.68244557e-03 -9.98540531e-01 -3.19133105e-04 -5.39803316e-02]\nImportant\n: Use your actual output quaternion values, not the example above. Each robot arm will produce different quaternion values based on its specific configuration.\n### 6.2 6.2 Test with New Quaternion ¶\nUse the output quaternion to test valid positions by modifying the test script, and REMEMBER TO UPDATE THE QUATERNION:\nimport\ntorch\nfrom\ncurobo.types.base\nimport\nTensorDeviceType\nfrom\ncurobo.types.math\nimport\nPose\nfrom\ncurobo.types.robot\nimport\nRobotConfig\nfrom\ncurobo.util_file\nimport\nget_robot_configs_path\n,\njoin_path\n,\nload_yaml\nfrom\ncurobo.wrap.reacher.ik_solver\nimport\nIKSolver\n,\nIKSolverConfig\ntensor_args\n=\nTensorDeviceType\n()\nconfig_file\n=\nload_yaml\n(\njoin_path\n(\nget_robot_configs_path\n(),\n\"franka.yml\"\n))\nurdf_file\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"urdf_path\"\n]\nbase_link\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"base_link\"\n]\nee_link\n=\nconfig_file\n[\n\"robot_cfg\"\n][\n\"kinematics\"\n][\n\"ee_link\"\n]\nrobot_cfg\n=\nRobotConfig\n.\nfrom_basic\n(\nurdf_file\n,\nbase_link\n,\nee_link\n,\ntensor_args\n)\nik_config\n=\nIKSolverConfig\n.\nload_from_robot_config\n(\nrobot_cfg\n,\nNone\n,\nnum_seeds\n=\n20\n,\nself_collision_check\n=\nFalse\n,\nself_collision_opt\n=\nFalse\n,\ntensor_args\n=\ntensor_args\n,\nuse_cuda_graph\n=\nTrue\n,\n)\nik_solver\n=\nIKSolver\n(\nik_config\n)\nx_values\n=\ntorch\n.\nlinspace\n(\n0.35\n,\n0.0\n,\n25\n)\n.\ntolist\n()\n+\ntorch\n.\nlinspace\n(\n0.35\n,\n0.7\n,\n25\n)\n.\ntolist\n()\ny_values\n=\ntorch\n.\nlinspace\n(\n0.25\n,\n0.0\n,\n25\n)\n.\ntolist\n()\n+\ntorch\n.\nlinspace\n(\n0.25\n,\n0.5\n,\n25\n)\n.\ntolist\n()\nz_values\n=\ntorch\n.\nlinspace\n(\n0.25\n,\n0.0\n,\n25\n)\n.\ntolist\n()\n+\ntorch\n.\nlinspace\n(\n0.25\n,\n0.5\n,\n25\n)\n.\ntolist\n()\n###### REMEMBER TO UPDATE THE QUATERNION ####\n#############################################\n# Update the quaternion from the debug output\nquaternion\n=\ntorch\n.\ntensor\n([[\n1.68244557e-03\n,\n-\n9.98540531e-01\n,\n-\n3.19133105e-04\n,\n-\n5.39803316e-02\n]],\ndevice\n=\n'cuda:0'\n)\n#############################################\nprint\n(\n\"Testing IK solutions for different positions:\"\n)\nprint\n(\n\"x, y, z, success\"\n)\nfor\nx\nin\nx_values\n:\nfor\ny\nin\ny_values\n:\nfor\nz\nin\nz_values\n:\ngoal\n=\nPose\n(\nposition\n=\ntorch\n.\ntensor\n([[\nfloat\n(\nx\n),\nfloat\n(\ny\n),\nfloat\n(\nz\n)]],\ndevice\n=\n'cuda:0'\n),\nquaternion\n=\nquaternion\n)\nresult\n=\nik_solver\n.\nsolve_single\n(\ngoal\n)\nif\nresult\n.\nsuccess\n.\nitem\n()\n==\nTrue\n:\nprint\n(\nf\n\"\n{\nx\n:\n.2f\n}\n,\n{\ny\n:\n.2f\n}\n,\n{\nz\n:\n.2f\n}\n,\n{\nresult\n.\nsuccess\n}\n\"\n)\nExpected output:\nx, y, z, success\n0.35, 0.24, 0.27, tensor([[True]], device='cuda:0')\n0.35, 0.24, 0.28, tensor([[True]], device='cuda:0')\n...\n### 6.3 6.3 Calculate global_trans_matrix ¶\nUpdate\nenvs/robot/planner.py\nwith a successful position:\ntarget_pose_p\n[\n0\n]\n+=\nself\n.\nframe_bias\n[\n0\n]\ntarget_pose_p\n[\n1\n]\n+=\nself\n.\nframe_bias\n[\n1\n]\ntarget_pose_p\n[\n2\n]\n+=\nself\n.\nframe_bias\n[\n2\n]\n# Update with successful position, remove debug print and target_pose_q\ntarget_pose_p\n=\nnp\n.\narray\n([\n0.35\n,\n0.24\n,\n0.27\n])\n# target_pose_q = np.array([1.0, 0.0, 0.0, 0.0])\n# print('[debug]: target_pose_q: ', target_pose_q)\ngoal_pose_of_gripper\n=\nCuroboPose\n.\nfrom_list\n(\nlist\n(\ntarget_pose_p\n)\n+\nlist\n(\ntarget_pose_q\n))\nReplace the entire\nplay_once(self)\nfunction in\nenvs/beat_block_hammer.py\nand\nUPDATE THE DELTA_MATRIX BELOW\n:\ndef\nplay_once\n(\nself\n):\n# Get the position of the block's functional point\nblock_pose\n=\nself\n.\nblock\n.\nget_functional_point\n(\n0\n,\n\"pose\"\n)\n.\np\n# Use left arm for testing\narm_tag\n=\n\"left\"\narm_tag\n=\nArmTag\n(\n'left'\n)\naction\n=\nAction\n(\narm_tag\n,\n'move'\n,\n[\n-\n0.05\n,\n0.\n,\n0.9\n])\nself\n.\nmove\n((\narm_tag\n,\n[\naction\n]))\nimport\ntransforms3d\nas\nt3d\nwhile\nTrue\n:\nleft_ee_global_pose_q\n=\nlist\n(\nself\n.\nrobot\n.\nleft_ee\n.\nglobal_pose\n.\nq\n)\nw_R_joint\n=\nt3d\n.\nquaternions\n.\nquat2mat\n(\nleft_ee_global_pose_q\n)\nw_R_aloha\n=\nt3d\n.\nquaternions\n.\nquat2mat\n(\naction\n[\n1\n][\n0\n]\n.\ntarget_pose\n[\n3\n:])\n######## REMEMBER TO UPDATE THE DELTA_MATRIX!!!! ####\n# Update this delta_matrix with your calculated value\ndelta_matrix\n=\nnp\n.\nmatrix\n([[\n0\n,\n0\n,\n1\n],[\n0\n,\n-\n1\n,\n0\n],[\n1\n,\n0\n,\n0\n]])\n#####################################################\nglobal_trans_matrix\n=\nw_R_joint\n.\nT\n@\nw_R_aloha\n@\ndelta_matrix\n.\nT\nprint\n(\nnp\n.\nround\n(\nglobal_trans_matrix\n))\nRun the simulation again:\nbash\ncollect_data.sh\nbeat_block_hammer\ndemo_randomized\n0\nExpected output:\n[[ 1. 0. 0.]\n[ 0. -1. 0.]\n[ 0. -0. -1.]]\nThis is your\nglobal_trans_matrix\n. Add it to your\nconfig.yml\n.\n### 6.4 6.4 Clean Up ¶\nRestore the modified files:\ngit\ncheckout\n--\nenvs/robot/planner.py\ngit\ncheckout\n--\nenvs/beat_block_hammer.py\nCongratulations!\nYour new embodiment configuration is now complete.\n## 7. Dual-Arm URDF Configuration ¶\nDual-arm URDFs have a slightly different structure:\n# Using ALOHA as an example\n- embodiments\n- aloha\n- config.yml # RoboTwin config file\n- curobo_left_tmp.yml # Left arm CuRobo config template\n- curobo_right_tmp.yml # Right arm CuRobo config template\n- collision_aloha_left.yml # Left arm collision annotations\n- collision_aloha_right.yml # Right arm collision annotations\n- urdf_files/... # URDF files and corresponding GLB, STL files, etc.\n### 7.1 Key Considerations for Dual-Arm Setup: ¶\nFrame Bias Configuration\n: In\ncurobo_left_tmp.yml\nand\ncurobo_right_tmp.yml\n, if your CuRobo config's\nrobot_cfg/kinematics/base_link\ndoesn't match the URDF's\nbase_link\n(e.g., using\nfl_base_link\nin ALOHA), you need\nplanner/frame_bias\n. This represents the translation vector from the URDF's\nbase_link\nto the CuRobo's\nbase_link\n(e.g.,\nfl_base_link\n). The same applies to the right arm.\nConfig.yml Settings\n: Set\ndual_arm: True\nin config.yml for dual-arm configurations.\nThis completes the embodiment configuration process. The setup allows RoboTwin to properly load and control your new robot embodiment in both simulation and planning contexts."}
{"id": "page-29", "contents": "Configuring New Camera - RoboTwin 2.0 Offical Document\nSkip to content\n# Configurating New Camera ¶\nModify\ntask_config/_camera_config.yml\n(\nGithub file\n), adding new camera new and configurate\nfov\n,\nh\nand\nw\n, such as:\nDemo_Camera:\nfovy: 56\nw: 224\nh: 224\nFinally, modify the camera type in the task config file.\ncamera:\nhead_camera_type: Demo_Camera\nwrist_camera_type: D435"}
{"id": "page-30", "contents": "Adjust Bottle - RoboTwin 2.0 Offical Document\n# Adjust Bottle\nDescription\n: Pick up the bottle on the table headup with the correct arm.\nAverage Steps\n: 147 (Aloha-AgileX, save_freq=15)\nObjects\n: 001_bottle\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n93%\n94%\n34%\n0%\n12%"}
{"id": "page-31", "contents": "Beat Block Hammer - RoboTwin 2.0 Offical Document\n# Beat Block Hammer\nDescription\n: There is a hammer and a block on the table, use the arm to grab the hammer and beat the block.\nAverage Steps\n: 113 (Aloha-AgileX, save_freq=15)\nObjects\n: 020_hammer, block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n64%\n93%\n98%\n15%\n90%"}
{"id": "page-32", "contents": "Blocks Ranking RGB - RoboTwin 2.0 Offical Document\n# Blocks Ranking RGB\nDescription\n: Place the red block, green block, and blue block in the order of red, green, and blue from left to right, placing in a row.\nAverage Steps\n: 466 (Aloha-AgileX, save_freq=15)\nObjects\n: block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n96%\n97%\n99%\n13%\n53%"}
{"id": "page-33", "contents": "Blocks Ranking Size - RoboTwin 2.0 Offical Document\n# Blocks Ranking Size\nDescription\n: There are three blocks on the table, the color of the blocks is random, move the blocks to the center of the table, and arrange them from largest to smallest, from left to right.\nAverage Steps\n: 466 (Aloha-AgileX, save_freq=15)\nObjects\n: block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n96%\n97%\n89%\n7%\n38%"}
{"id": "page-34", "contents": "Click Alarmclock - RoboTwin 2.0 Offical Document\n# Click Alarmclock\nDescription\n: Click the alarm clock's center of the top side button on the table.\nAverage Steps\n: 85 (Aloha-AgileX, save_freq=15)\nObjects\n: 046_alarm-clock\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n92%\n99%\n100%\n0%\n95%"}
{"id": "page-35", "contents": "Click Bell - RoboTwin 2.0 Offical Document\n# Click Bell\nDescription\n: Click the bell's top center on the table.\nAverage Steps\n: 85 (Aloha-AgileX, save_freq=15)\nObjects\n: 050_bell\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n100%\n100%\n100%\n91%\n100%"}
{"id": "page-36", "contents": "Dump Bin Bigbin - RoboTwin 2.0 Offical Document\n# Dump Bin Bigbin\nDescription\n: Grab the small bin and pour the balls into the big bin.\nAverage Steps\n: 265 (Aloha-AgileX, save_freq=15)\nObjects\n: 011_dustbin, 063_tabletrashbin\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n84%\n100%\n84%\n9%\n80%"}
{"id": "page-37", "contents": "Grab Roller - RoboTwin 2.0 Offical Document\n# Grab Roller\nDescription\n: Use both arms to grab the roller on the table.\nAverage Steps\n: 94 (Aloha-AgileX, save_freq=15)\nObjects\n: 102_roller\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n95%\n69%\n99%\n7%\n81%"}
{"id": "page-38", "contents": "Handover Block - RoboTwin 2.0 Offical Document\n# Handover Block\nDescription\n: Use the left arm to grasp the red block on the table, handover it to the right arm and place it on the blue pad.\nAverage Steps\n: 283 (Aloha-AgileX, save_freq=15)\nObjects\n: block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n83%\n81%\n0%\n44%\n0%"}
{"id": "page-39", "contents": "Handover Mic - RoboTwin 2.0 Offical Document\n# Handover Mic\nDescription\n: Use one arm to grasp the microphone on the table and handover it to the other arm.\nAverage Steps\n: 223 (Aloha-AgileX, save_freq=15)\nObjects\n: 018_microphone\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n87%\n98%\n84%\n65%\n14%"}
{"id": "page-40", "contents": "Hanging Mug - RoboTwin 2.0 Offical Document\n# Hanging Mug\nDescription\n: Use left arm to pick the mug on the table, rotate the mug and put the mug down in the middle of the table, use the right arm to pick the mug and hang it onto the rack.\nAverage Steps\n: 340 (Aloha-AgileX, save_freq=15)\nObjects\n: 039_mug, 040_rack\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n63%\n73%\n11%\n0%\n11%"}
{"id": "page-41", "contents": "Lift Pot - RoboTwin 2.0 Offical Document\n# Lift Pot\nDescription\n: Use arms to lift the pot.\nAverage Steps\n: 112 (Aloha-AgileX, save_freq=15)\nObjects\n: 060_kitchenpot\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n27%\n50%\n36%\n31%\n40%"}
{"id": "page-42", "contents": "Move Can Pot - RoboTwin 2.0 Offical Document\n# Move Can Pot\nDescription\n: There is a can and a pot on the table, use one arm to pick up the can and move it to beside the pot.\nAverage Steps\n: 151 (Aloha-AgileX, save_freq=15)\nObjects\n: 060_kitchenpot, 105_sauce-can\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n93%\n65%\n92%\n96%\n99%"}
{"id": "page-43", "contents": "Move Playingcard Away - RoboTwin 2.0 Offical Document\n# Move Playingcard Away\nDescription\n: Use the arm to pick up the playing card and move it away from the table. For example, if the playing card is on the outward side of the table, you should move it further outward side of the table.\nAverage Steps\n: 120 (Aloha-AgileX, save_freq=15)\nObjects\n: 081_playingcards\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n99%\n100%\n100%\n63%\n66%"}
{"id": "page-44", "contents": "Move Stapler Pad - RoboTwin 2.0 Offical Document\n# Move Stapler Pad\nDescription\n: Use appropriate arm to move the stapler to a colored mat.\nAverage Steps\n: 152 (Aloha-AgileX, save_freq=15)\nObjects\n: 048_stapler\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n92%\n96%\n89%\n13%\n75%"}
{"id": "page-45", "contents": "Open Laptop - RoboTwin 2.0 Offical Document\n# Open Laptop\nDescription\n: Use one arm to open the laptop.\nAverage Steps\n: 258 (Aloha-AgileX, save_freq=15)\nObjects\n: 015_laptop\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n82%\n92%\n77%\n23%\n51%"}
{"id": "page-46", "contents": "Open Microwave - RoboTwin 2.0 Offical Document\n# Open Microwave\nDescription\n: Use one arm to open the microwave.\nAverage Steps\n: 537 (Aloha-AgileX, save_freq=15)\nObjects\n: 044_microwave\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n96%\n80%\n59%\n2%\n23%"}
{"id": "page-47", "contents": "Pick Diverse Bottles - RoboTwin 2.0 Offical Document\n# Pick Diverse Bottles\nDescription\n: Pick up one bottle with one arm, and pick up another bottle with the other arm.\nAverage Steps\n: 122 (Aloha-AgileX, save_freq=15)\nObjects\n: 001_bottle\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n51%\n2%\n0%\n27%\n4%"}
{"id": "page-48", "contents": "Pick Dual Bottles - RoboTwin 2.0 Offical Document\n# Pick Dual Bottles\nDescription\n: Pick up one bottle with one arm, and pick up another bottle with the other arm.\nAverage Steps\n: 127 (Aloha-AgileX, save_freq=15)\nObjects\n: 001_bottle\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n92%\n6%\n0%\n81%\n7%"}
{"id": "page-49", "contents": "Place A2B Left - RoboTwin 2.0 Offical Document\n# Place A2B Left\nDescription\n: Use appropriate arm to place object A on the left of object B.\nAverage Steps\n: 155 (Aloha-AgileX, save_freq=15)\nObjects\n: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 086_woodenblock, 107_soap, 112_tea-box, 113_coffee-box\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n80%\n88%\n64%\n28%\n76%"}
{"id": "page-50", "contents": "Place A2B Right - RoboTwin 2.0 Offical Document\n# Place A2B Right\nDescription\n: Use appropriate arm to place object A on the right of object B.\nAverage Steps\n: 145 (Aloha-AgileX, save_freq=15)\nObjects\n: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 086_woodenblock, 107_soap, 112_tea-box, 113_coffee-box\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n81%\n82%\n64%\n31%\n66%"}
{"id": "page-51", "contents": "Place Bread Basket - RoboTwin 2.0 Offical Document\n# Place Bread Basket\nDescription\n: If there is one bread on the table, use one arm to grab the bread and put it in the basket, if there are two breads on the table, use two arms to simultaneously grab up two breads and put them in the basket.\nAverage Steps\n: 231 (Aloha-AgileX, save_freq=15)\nObjects\n: 075_bread, 076_breadbasket\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n89%\n88%\n62%\n1%\n67%"}
{"id": "page-52", "contents": "Place Bread Skillet - RoboTwin 2.0 Offical Document\n# Place Bread Skillet\nDescription\n: If there is one bread on the table, use one arm to grab the bread and put it into the skillet.\nAverage Steps\n: 162 (Aloha-AgileX, save_freq=15)\nObjects\n: 075_bread, 106_skillet\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n34%\n26%\n42%\n0%\n37%"}
{"id": "page-53", "contents": "Place Burger Fries - RoboTwin 2.0 Offical Document\n# Place Burger Fries\nDescription\n: Use dual arm to pick the hamburg and frenchfries and put them onto the tray.\nAverage Steps\n: 242 (Aloha-AgileX, save_freq=15)\nObjects\n: 005_french-fries, 006_hamburg, 008_tray\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n97%\n98%\n80%\n36%\n74%"}
{"id": "page-54", "contents": "Place Can Basket - RoboTwin 2.0 Offical Document\n# Place Can Basket\nDescription\n: Use one arm to pick up the can, put it into the basket, and use another arm to lift the basket\nAverage Steps\n: 255 (Aloha-AgileX, save_freq=15)\nObjects\n: 071_can, 110_basket\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n70%\n28%\n61%\n0%\n3%"}
{"id": "page-55", "contents": "Place Cans Plasticbox - RoboTwin 2.0 Offical Document\n# Place Cans Plasticbox\nDescription\n: Use dual arm to pick and place cans into plasticbox.\nAverage Steps\n: 289 (Aloha-AgileX, save_freq=15)\nObjects\n: 062_plasticbox, 071_can\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n100%\n96%\n85%\n0%\n82%"}
{"id": "page-56", "contents": "Place Container Plate - RoboTwin 2.0 Offical Document\n# Place Container Plate\nDescription\n: Place the container onto the plate.\nAverage Steps\n: 156 (Aloha-AgileX, save_freq=15)\nObjects\n: 002_bowl, 003_plate, 021_cup\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n89%\n86%\n86%\n37%\n81%"}
{"id": "page-57", "contents": "Place Dual Shoes - RoboTwin 2.0 Offical Document\n# Place Dual Shoes\nDescription\n: Use both arms to pick up the two shoes on the table and put them in the shoebox, with the shoe tip pointing to the left.\nAverage Steps\n: 228 (Aloha-AgileX, save_freq=15)\nObjects\n: 007_shoe-box, 041_shoe\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n77%\n31%\n41%\n1%\n32%"}
{"id": "page-58", "contents": "Place Empty Cup - RoboTwin 2.0 Offical Document\n# Place Empty Cup\nDescription\n: Use an arm to place the empty cup on the coaster.\nAverage Steps\n: 174 (Aloha-AgileX, save_freq=15)\nObjects\n: 019_coaster, 021_cup\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n92%\n100%\n100%\n4%\n100%"}
{"id": "page-59", "contents": "Place Fan - RoboTwin 2.0 Offical Document\n# Place Fan\nDescription\n: Grab the fan and place it on a colored mat, and make sure the fan is facing the robot.\nAverage Steps\n: 148 (Aloha-AgileX, save_freq=15)\nObjects\n: 099_fan, block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n95%\n93%\n83%\n0%\n65%"}
{"id": "page-60", "contents": "Place Mouse Pad - RoboTwin 2.0 Offical Document\n# Place Mouse Pad\nDescription\n: Grab the mouse and place it on a colored mat.\nAverage Steps\n: 149 (Aloha-AgileX, save_freq=15)\nObjects\n: 047_mouse, block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n99%\n89%\n100%\n23%\n73%"}
{"id": "page-61", "contents": "Place Object Basket - RoboTwin 2.0 Offical Document\n# Place Object Basket\nDescription\n: Use one arm to grab the target object and put it in the basket, then use the other arm to grab the basket, and finally move the basket slightly away.\nAverage Steps\n: 252 (Aloha-AgileX, save_freq=15)\nObjects\n: 057_toycar, 081_playingcards, 110_basket\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n74%\n14%\n61%\n0%\n7%"}
{"id": "page-62", "contents": "Place Object Scale - RoboTwin 2.0 Offical Document\n# Place Object Scale\nDescription\n: Use one arm to grab the object and put it on the scale.\nAverage Steps\n: 146 (Aloha-AgileX, save_freq=15)\nObjects\n: 047_mouse, 048_stapler, 050_bell, 072_electronicscale\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n78%\n92%\n82%\n2%\n76%"}
{"id": "page-63", "contents": "Place Object Stand - RoboTwin 2.0 Offical Document\n# Place Object Stand\nDescription\n: Use appropriate arm to place the object on the stand.\nAverage Steps\n: 138 (Aloha-AgileX, save_freq=15)\nObjects\n: 047_mouse, 048_stapler, 050_bell, 057_toycar, 073_rubikscube, 074_displaystand, 079_remotecontrol\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n97%\n99%\n81%\n9%\n92%"}
{"id": "page-64", "contents": "Place Phone Stand - RoboTwin 2.0 Offical Document\n# Place Phone Stand\nDescription\n: Pick up the phone and put it on the phone stand.\nAverage Steps\n: 130 (Aloha-AgileX, save_freq=15)\nObjects\n: 077_phone, 078_phonestand\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n66%\n78%\n45%\n53%\n49%"}
{"id": "page-65", "contents": "Place Shoe - RoboTwin 2.0 Offical Document\n# Place Shoe\nDescription\n: Use one arm to grab the shoe from the table and place it on the mat.\nAverage Steps\n: 178 (Aloha-AgileX, save_freq=15)\nObjects\n: 041_shoe, block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n84%\n85%\n74%\n7%\n91%"}
{"id": "page-66", "contents": "Press Stapler - RoboTwin 2.0 Offical Document\n# Press Stapler\nDescription\n: Use one arm to press the stapler.\nAverage Steps\n: 141 (Aloha-AgileX, save_freq=15)\nObjects\n: 048_stapler\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n98%\n96%\n100%\n59%\n72%"}
{"id": "page-67", "contents": "Put Bottles Dustbin - RoboTwin 2.0 Offical Document\n# Put Bottles Dustbin\nDescription\n: Use arms to grab the bottles and put them into the dustbin to the left of the table.\nAverage Steps\n: 637 (Aloha-AgileX, save_freq=15)\nObjects\n: 011_dustbin, 114_bottle\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n71%\n1%\n0%\n56%\n0%"}
{"id": "page-68", "contents": "Put Object Cabinet - RoboTwin 2.0 Offical Document\n# Put Object Cabinet\nDescription\n: Use one arm to open the cabinet's drawer, and use another arm to put the object on the table to the drawer.\nAverage Steps\n: 274 (Aloha-AgileX, save_freq=15)\nObjects\n: 036_cabinet, 047_mouse, 048_stapler, 057_toycar, 073_rubikscube, 075_bread, 077_phone, 081_playingcards, 107_soap, 112_tea-box, 113_coffee-box\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n14%\n24%\n55%\n0%\n0%"}
{"id": "page-69", "contents": "Rotate QRcode - RoboTwin 2.0 Offical Document\n# Rotate QRcode\nDescription\n: Use arm to catch the qrcode board on the table, pick it up and rotate to let the qrcode face towards the robot.\nAverage Steps\n: 155 (Aloha-AgileX, save_freq=15)\nObjects\n: 070_paymentsign\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n75%\n74%\n94%\n0%\n67%"}
{"id": "page-70", "contents": "Scan Object - RoboTwin 2.0 Offical Document\n# Scan Object\nDescription\n: Use one arm to pick the scanner and use the other arm to pick the object, and use the scanner to scan the object.\nAverage Steps\n: 170 (Aloha-AgileX, save_freq=15)\nObjects\n: 024_scanner, 112_tea-box\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n4%\n45%\n26%\n0%\n19%"}
{"id": "page-71", "contents": "Shake Bottle Horizontally - RoboTwin 2.0 Offical Document\n# Shake Bottle Horizontally\nDescription\n: Shake the bottle horizontally with proper arm.\nAverage Steps\n: 276 (Aloha-AgileX, save_freq=15)\nObjects\n: 001_bottle\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n90%\n94%\n85%\n74%\n98%"}
{"id": "page-72", "contents": "Shake Bottle - RoboTwin 2.0 Offical Document\n# Shake Bottle\nDescription\n: Shake the bottle with proper arm.\nAverage Steps\n: 246 (Aloha-AgileX, save_freq=15)\nObjects\n: 001_bottle\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n89%\n94%\n85%\n74%\n97%"}
{"id": "page-73", "contents": "Stack Blocks Three - RoboTwin 2.0 Offical Document\n# Stack Blocks Three\nDescription\n: There are three blocks on the table, the color of the blocks is red, green and blue. Move the blocks to the center of the table, and stack the blue block on the green block, and the green block on the red block.\nAverage Steps\n: 481 (Aloha-AgileX, save_freq=15)\nObjects\n: block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n94%\n96%\n80%\n0%\n51%"}
{"id": "page-74", "contents": "Stack Blocks Two - RoboTwin 2.0 Offical Document\n# Stack Blocks Two\nDescription\n: There are two blocks on the table, the color of the blocks is red, green. Move the blocks to the center of the table, and stack the geen block on the red block.\nAverage Steps\n: 316 (Aloha-AgileX, save_freq=15)\nObjects\n: block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n98%\n99%\n96%\n2%\n68%"}
{"id": "page-75", "contents": "Stack Bowls Three - RoboTwin 2.0 Offical Document\n# Stack Bowls Three\nDescription\n: Stack the three bowls on top of each other.\nAverage Steps\n: 476 (Aloha-AgileX, save_freq=15)\nObjects\n: 002_bowl\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n43%\n57%\n82%\n0%\n81%"}
{"id": "page-76", "contents": "Stack Bowls Two - RoboTwin 2.0 Offical Document\n# Stack Bowls Two\nDescription\n: Stack the two bowls on top of each other.\nAverage Steps\n: 313 (Aloha-AgileX, save_freq=15)\nObjects\n: 002_bowl\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n78%\n82%\n88%\n4%\n94%"}
{"id": "page-77", "contents": "Stamp Seal - RoboTwin 2.0 Offical Document\n# Stamp Seal\nDescription\n: Grab the stamp and stamp onto the specific color mat.\nAverage Steps\n: 151 (Aloha-AgileX, save_freq=15)\nObjects\n: 100_seal, block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n56%\n91%\n4%\n37%\n100%"}
{"id": "page-78", "contents": "Turn Switch - RoboTwin 2.0 Offical Document\n# Turn Switch\nDescription\n: Use the robotic arm to click the switch.\nAverage Steps\n: 95 (Aloha-AgileX, save_freq=15)\nObjects\n: 056_switch\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n74%\n3%\n36%\n81%\n10%"}
{"id": "page-79", "contents": "Move Pillbottle Pad - RoboTwin 2.0 Offical Document\n# Move Pillbottle Pad\nDescription\n: Use one arm to pick the pillbottle and place it onto the pad.\nAverage Steps\n: 147 (Aloha-AgileX, save_freq=15)\nObjects\n: 080_pillbottle, block\nEmbodiments\nAloha-AgileX\nARX-X5\nFranka-Panda\nPiper\nUR5-Wsg\nData Generation Success Rate\n67%\n90%\n69%\n47%\n86%"}
{"id": "page-80", "contents": "001_Bottle - RoboTwin 2.0 Offical Document\n# 001_Bottle\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9\nbase10\nbase11\nbase12\nbase13\nbase14\nbase15\nbase16\nbase17\nbase18\nbase19\nbase20\nbase21\nbase22"}
{"id": "page-81", "contents": "002_Bowl - RoboTwin 2.0 Offical Document\n# 002_Bowl\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7"}
{"id": "page-82", "contents": "003_Plate - RoboTwin 2.0 Offical Document\n# 003_Plate\nbase0"}
{"id": "page-83", "contents": "004_Fluted-Block - RoboTwin 2.0 Offical Document\n# 004_Fluted-Block\nbase0\nbase1"}
{"id": "page-84", "contents": "005_French-Fries - RoboTwin 2.0 Offical Document\n# 005_French-Fries\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-85", "contents": "006_Hamburg - RoboTwin 2.0 Offical Document\n# 006_Hamburg\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-86", "contents": "007_Shoe-Box - RoboTwin 2.0 Offical Document\n# 007_Shoe-Box\nbase0"}
{"id": "page-87", "contents": "008_Tray - RoboTwin 2.0 Offical Document\n# 008_Tray\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-88", "contents": "009_Kettle - RoboTwin 2.0 Offical Document\n# 009_Kettle\nbase0\nbase1\nbase2"}
{"id": "page-89", "contents": "010_Pen - RoboTwin 2.0 Offical Document\n# 010_Pen\nMissing images for the pen object."}
{"id": "page-90", "contents": "011_Dustbin - RoboTwin 2.0 Offical Document\n# 011_Dustbin\nbase0"}
{"id": "page-91", "contents": "012_Plant-Pot - RoboTwin 2.0 Offical Document\n# 012_Plant-Pot\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-92", "contents": "013_Dumbbell-Rack - RoboTwin 2.0 Offical Document\n# 013_Dumbbell-Rack\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-93", "contents": "014_Bookcase - RoboTwin 2.0 Offical Document\n# 014_Bookcase\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-94", "contents": "015_Laptop - RoboTwin 2.0 Offical Document\n# 015_Laptop\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9\nbase10"}
{"id": "page-95", "contents": "016_Oven - RoboTwin 2.0 Offical Document\n# 016_Oven\nMissing images for the oven object."}
{"id": "page-96", "contents": "017_Calculator - RoboTwin 2.0 Offical Document\n# 017_Calculator\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-97", "contents": "018_Microphone - RoboTwin 2.0 Offical Document\n# 018_Microphone\nbase0\nbase1\nbase4\nbase5"}
{"id": "page-98", "contents": "019_Coaster - RoboTwin 2.0 Offical Document\n# 019_Coaster\nbase0"}
{"id": "page-99", "contents": "020_Hammer - RoboTwin 2.0 Offical Document\n# 020_Hammer\nbase0"}
{"id": "page-100", "contents": "021_Cup - RoboTwin 2.0 Offical Document\n# 021_Cup\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9\nbase10\nbase11\nbase12"}
{"id": "page-101", "contents": "022_Cup-With-Liquid - RoboTwin 2.0 Offical Document\n# 022_Cup-With-Liquid\nbase0"}
{"id": "page-102", "contents": "023_Tissue-Box - RoboTwin 2.0 Offical Document\n# 023_Tissue-Box\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-103", "contents": "024_Scanner - RoboTwin 2.0 Offical Document\n# 024_Scanner\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-104", "contents": "025_Chips-Tub - RoboTwin 2.0 Offical Document\n# 025_Chips-Tub\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-105", "contents": "026_Pet-Collar - RoboTwin 2.0 Offical Document\n# 026_Pet-Collar\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-106", "contents": "027_Table-Tennis - RoboTwin 2.0 Offical Document\n# 027_Table-Tennis\nbase0\nbase1"}
{"id": "page-107", "contents": "028_Roll-Paper - RoboTwin 2.0 Offical Document\n# 028_Roll-Paper\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-108", "contents": "029_Olive-Oil - RoboTwin 2.0 Offical Document\n# 029_Olive-Oil\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-109", "contents": "030_Drill - RoboTwin 2.0 Offical Document\n# 030_Drill\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-110", "contents": "031_Jam-Jar - RoboTwin 2.0 Offical Document\n# 031_Jam-Jar\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-111", "contents": "032_Screwdriver - RoboTwin 2.0 Offical Document\n# 032_Screwdriver\nbase0"}
{"id": "page-112", "contents": "033_Fork - RoboTwin 2.0 Offical Document\n# 033_Fork\nbase0"}
{"id": "page-113", "contents": "034_Knife - RoboTwin 2.0 Offical Document\n# 034_Knife\nbase0"}
{"id": "page-114", "contents": "035_Apple - RoboTwin 2.0 Offical Document\n# 035_Apple\nbase0\nbase1"}
{"id": "page-115", "contents": "036_Cabinet - RoboTwin 2.0 Offical Document\n# 036_Cabinet\nbase0"}
{"id": "page-116", "contents": "037_Box - RoboTwin 2.0 Offical Document\n# 037_Box\nbase0"}
{"id": "page-117", "contents": "038_Milk-Box - RoboTwin 2.0 Offical Document\n# 038_Milk-Box\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-118", "contents": "039_Mug - RoboTwin 2.0 Offical Document\n# 039_Mug\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9\nbase10\nbase11\nbase12"}
{"id": "page-119", "contents": "040_Rack - RoboTwin 2.0 Offical Document\n# 040_Rack\nbase0"}
{"id": "page-120", "contents": "041_Shoe - RoboTwin 2.0 Offical Document\n# 041_Shoe\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9"}
{"id": "page-121", "contents": "042_Wooden_Box - RoboTwin 2.0 Offical Document\n# 042_Wooden_Box\nbase0"}
{"id": "page-122", "contents": "043_Book - RoboTwin 2.0 Offical Document\n# 043_Book\nbase0\nbase1"}
{"id": "page-123", "contents": "044_Microwave - RoboTwin 2.0 Offical Document\n# 044_Microwave\nbase0\nbase1"}
{"id": "page-124", "contents": "045_Sand-Clock - RoboTwin 2.0 Offical Document\n# 045_Sand-Clock\nbase0\nbase1\nbase3"}
{"id": "page-125", "contents": "046_Alarm-Clock - RoboTwin 2.0 Offical Document\n# 046_Alarm-Clock\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-126", "contents": "047_Mouse - RoboTwin 2.0 Offical Document\n# 047_Mouse\nbase0\nbase1\nbase2"}
{"id": "page-127", "contents": "048_Stapler - RoboTwin 2.0 Offical Document\n# 048_Stapler\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-128", "contents": "049_Shampoo - RoboTwin 2.0 Offical Document\n# 049_Shampoo\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7"}
{"id": "page-129", "contents": "050_Bell - RoboTwin 2.0 Offical Document\n# 050_Bell\nbase0\nbase1"}
{"id": "page-130", "contents": "051_Candlestick - RoboTwin 2.0 Offical Document\n# 051_Candlestick\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-131", "contents": "052_Dumbbell - RoboTwin 2.0 Offical Document\n# 052_Dumbbell\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-132", "contents": "053_Teanet - RoboTwin 2.0 Offical Document\n# 053_Teanet\nbase1\nbase4\nbase5\nbase6\nbase7"}
{"id": "page-133", "contents": "054_Baguette - RoboTwin 2.0 Offical Document\n# 054_Baguette\nbase2\nbase3"}
{"id": "page-134", "contents": "055_Small-Speaker - RoboTwin 2.0 Offical Document\n# 055_Small-Speaker\nbase1\nbase2\nbase3"}
{"id": "page-135", "contents": "056_Switch - RoboTwin 2.0 Offical Document\n# 056_Switch\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7"}
{"id": "page-136", "contents": "057_Toycar - RoboTwin 2.0 Offical Document\n# 057_Toycar\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-137", "contents": "058_Markpen - RoboTwin 2.0 Offical Document\n# 058_Markpen\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-138", "contents": "059_Pencup - RoboTwin 2.0 Offical Document\n# 059_Pencup\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-139", "contents": "060_Kitchenpot - RoboTwin 2.0 Offical Document\n# 060_Kitchenpot\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7"}
{"id": "page-140", "contents": "061_Battery - RoboTwin 2.0 Offical Document\n# 061_Battery\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-141", "contents": "062_Plasticbox - RoboTwin 2.0 Offical Document\n# 062_Plasticbox\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9\nbase10"}
{"id": "page-142", "contents": "063_Tabletrashbin - RoboTwin 2.0 Offical Document\n# 063_Tabletrashbin\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7\nbase8\nbase9\nbase10"}
{"id": "page-143", "contents": "064_Msg - RoboTwin 2.0 Offical Document\n# 064_Msg\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-144", "contents": "065_Soy-Sauce - RoboTwin 2.0 Offical Document\n# 065_Soy-Sauce\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-145", "contents": "066_Vinegar - RoboTwin 2.0 Offical Document\n# 066_Vinegar\nbase0\nbase1\nbase2"}
{"id": "page-146", "contents": "067_Steamer - RoboTwin 2.0 Offical Document\n# 067_Steamer\nbase0\nbase1\nbase2"}
{"id": "page-147", "contents": "068_Boxdrink - RoboTwin 2.0 Offical Document\n# 068_Boxdrink\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-148", "contents": "069_Vagetable - RoboTwin 2.0 Offical Document\n# 069_Vagetable\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-149", "contents": "070_Paymentsign - RoboTwin 2.0 Offical Document\n# 070_Paymentsign\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-150", "contents": "071_Can - RoboTwin 2.0 Offical Document\n# 071_Can\nbase0\nbase1\nbase2\nbase3\nbase5\nbase6"}
{"id": "page-151", "contents": "072_Electronicscale - RoboTwin 2.0 Offical Document\n# 072_Electronicscale\nbase0\nbase1\nbase2\nbase5\nbase6"}
{"id": "page-152", "contents": "073_Rubikscube - RoboTwin 2.0 Offical Document\n# 073_Rubikscube\nbase0\nbase1\nbase2"}
{"id": "page-153", "contents": "074_Displaystand - RoboTwin 2.0 Offical Document\n# 074_Displaystand\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-154", "contents": "075_Bread - RoboTwin 2.0 Offical Document\n# 075_Bread\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-155", "contents": "076_Breadbasket - RoboTwin 2.0 Offical Document\n# 076_Breadbasket\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-156", "contents": "077_Phone - RoboTwin 2.0 Offical Document\n# 077_Phone\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-157", "contents": "078_Phonestand - RoboTwin 2.0 Offical Document\n# 078_Phonestand\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-158", "contents": "079_Remotecontrol - RoboTwin 2.0 Offical Document\n# 079_Remotecontrol\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-159", "contents": "080_Pillbottle - RoboTwin 2.0 Offical Document\n# 080_Pillbottle\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-160", "contents": "081_Playingcards - RoboTwin 2.0 Offical Document\n# 081_Playingcards\nbase0\nbase1\nbase2"}
{"id": "page-161", "contents": "082_Smallshovel - RoboTwin 2.0 Offical Document\n# 082_Smallshovel\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-162", "contents": "083_Brush - RoboTwin 2.0 Offical Document\n# 083_Brush\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-163", "contents": "084_Woodenmallet - RoboTwin 2.0 Offical Document\n# 084_Woodenmallet\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-164", "contents": "085_Gong - RoboTwin 2.0 Offical Document\n# 085_Gong\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-165", "contents": "086_Woodenblock - RoboTwin 2.0 Offical Document\n# 086_Woodenblock\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-166", "contents": "087_Waterer - RoboTwin 2.0 Offical Document\n# 087_Waterer\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6\nbase7"}
{"id": "page-167", "contents": "088_Wineglass - RoboTwin 2.0 Offical Document\n# 088_Wineglass\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-168", "contents": "089_Globe - RoboTwin 2.0 Offical Document\n# 089_Globe\nbase2\nbase3"}
{"id": "page-169", "contents": "090_Trophy - RoboTwin 2.0 Offical Document\n# 090_Trophy\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-170", "contents": "091_Kettle - RoboTwin 2.0 Offical Document\n# 091_Kettle\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-171", "contents": "092_Notebook - RoboTwin 2.0 Offical Document\n# 092_Notebook\nbase0\nbase1\nbase2"}
{"id": "page-172", "contents": "093_Brush-Pen - RoboTwin 2.0 Offical Document\n# 093_Brush-Pen\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-173", "contents": "094_Rest - RoboTwin 2.0 Offical Document\n# 094_Rest\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-174", "contents": "095_Glue - RoboTwin 2.0 Offical Document\n# 095_Glue\nbase0\nbase1\nbase2\nbase4\nbase5\nbase6"}
{"id": "page-175", "contents": "096_Cleaner - RoboTwin 2.0 Offical Document\n# 096_Cleaner\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-176", "contents": "097_Screen - RoboTwin 2.0 Offical Document\n# 097_Screen\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-177", "contents": "098_Speaker - RoboTwin 2.0 Offical Document\n# 098_Speaker\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-178", "contents": "099_Fan - RoboTwin 2.0 Offical Document\n# 099_Fan\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-179", "contents": "100_Seal - RoboTwin 2.0 Offical Document\n# 100_Seal\nbase0\nbase1\nbase2\nbase3\nbase4\nbase6"}
{"id": "page-180", "contents": "101_Milk-Tea - RoboTwin 2.0 Offical Document\n# 101_Milk-Tea\nbase0\nbase1\nbase2\nbase4\nbase5\nbase6"}
{"id": "page-181", "contents": "102_Roller - RoboTwin 2.0 Offical Document\n# 102_Roller\nbase0\nbase1\nbase2"}
{"id": "page-182", "contents": "103_Fruit - RoboTwin 2.0 Offical Document\n# 103_Fruit\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-183", "contents": "104_Board - RoboTwin 2.0 Offical Document\n# 104_Board\nbase0\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-184", "contents": "105_Sauce-Can - RoboTwin 2.0 Offical Document\n# 105_Sauce-Can\nbase0\nbase2\nbase4\nbase5\nbase6"}
{"id": "page-185", "contents": "106_Skillet - RoboTwin 2.0 Offical Document\n# 106_Skillet\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-186", "contents": "107_Soap - RoboTwin 2.0 Offical Document\n# 107_Soap\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-187", "contents": "108_Block - RoboTwin 2.0 Offical Document\n# 108_Block\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-188", "contents": "109_Hydrating-Oil - RoboTwin 2.0 Offical Document\n# 109_Hydrating-Oil\nbase0\nbase1\nbase2\nbase5"}
{"id": "page-189", "contents": "110_Basket - RoboTwin 2.0 Offical Document\n# 110_Basket\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-190", "contents": "111_Callbell - RoboTwin 2.0 Offical Document\n# 111_Callbell\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-191", "contents": "112_Tea-Box - RoboTwin 2.0 Offical Document\n# 112_Tea-Box\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5"}
{"id": "page-192", "contents": "113_Coffee-Box - RoboTwin 2.0 Offical Document\n# 113_Coffee-Box\nbase0\nbase1\nbase2\nbase3\nbase4\nbase5\nbase6"}
{"id": "page-193", "contents": "114_Bottle - RoboTwin 2.0 Offical Document\n# 114_Bottle\nbase1\nbase2\nbase3\nbase4"}
{"id": "page-194", "contents": "115_Perfume - RoboTwin 2.0 Offical Document\n# 115_Perfume\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-195", "contents": "116_Keyboard - RoboTwin 2.0 Offical Document\n# 116_Keyboard\nbase0\nbase1\nbase2\nbase3"}
{"id": "page-196", "contents": "117_Whiteboard-Eraser - RoboTwin 2.0 Offical Document\n# 117_Whiteboard-Eraser\nbase0"}
{"id": "page-197", "contents": "118_Tooth-Paste - RoboTwin 2.0 Offical Document\n# 118_Tooth-Paste\nbase0"}
{"id": "page-198", "contents": "119_Mini-Chalkboard - RoboTwin 2.0 Offical Document\n# 119_Mini-Chalkboard\nbase0"}
{"id": "page-199", "contents": "120_Plant - RoboTwin 2.0 Offical Document\n# 120_Plant\nbase0"}
{"id": "page-201", "contents": "Model data info - RoboTwin 2.0 Offical Document\n# Model data info\ncenter\n: 物体中心，相对于物体资产坐标的偏移向量（x y z）: 1*3 matrix\nextents\n: 资产长宽高（x y z 坐标轴上的长度）: 1*3 matrix\nscale\n: 场景中对实际物体资产的缩放（x y z方向上）: 1*3 matrix\ntarget_pose\n: 已被废除，无含义，一般不标定\ncontact_points_pose\n: list列表，列表元素为4*4旋转+平移矩阵，表示抓取点与物体中心坐标的偏移: n*4*4 matrix\ntransform_matrix\n: 不重要，一般不标定\nfunctional_matrix\n: list列表，列表元素为4*4旋转+平移矩阵，表示功能点与物体中心坐标的偏移: n*4*4 matrix\norientation_point\n: 方向点，一般不标定\ncontact_points_group\n: 抓取点分组，原因为有些抓取点xyz坐标相同，但是轴的方向不同，我们把这部分抓取点分到同个group内，方便一些操作\ncontact_points_mask\n: 自动生成，无需标定（似乎被废除了）\ncontact_points_discription\n: 抓取点描述: 1*n str\nfunctional_point_discription\n: 功能点描述: 1*n str\norientation_point_discription\n： 方向点描述: 1*n str\n注意\n：描述标定一般通过直接更改json文件来标定，不是强格式标定数据，也可通过其他表示方法外部标定。"}
